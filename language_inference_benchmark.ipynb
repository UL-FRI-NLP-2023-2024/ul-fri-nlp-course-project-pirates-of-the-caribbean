{"cells":[{"cell_type":"code","execution_count":230,"metadata":{"execution":{"iopub.execute_input":"2024-04-29T16:33:50.298939Z","iopub.status.busy":"2024-04-29T16:33:50.298089Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"]}],"source":["import wandb\n","WANDB_API = \"\"\n","wandb.login(key=WANDB_API)\n","\n","TRAIN_PATH = 'SI-NLI/train.tsv'\n","TEST_PATH = 'SI-NLI/test.tsv'"]},{"cell_type":"code","execution_count":231,"metadata":{"trusted":true},"outputs":[],"source":["import torch\n","import time\n","import pandas as pd\n","import numpy as np  \n","import os\n","from transformers import (\n","    AutoModelForSequenceClassification,\n","    AutoTokenizer,\n","    DataCollatorWithPadding,\n","    Trainer,\n","    TrainingArguments,\n",")\n","from sklearn.model_selection import train_test_split\n","from datasets import Dataset\n","from datasets import load_dataset, load_metric\n","from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, TaskType, PrefixTuningConfig, IA3Config\n","from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score"]},{"cell_type":"code","execution_count":232,"metadata":{"trusted":true},"outputs":[],"source":["def load_data():\n","    train = pd.read_csv(TRAIN_PATH, sep='\\t')\n","    test = pd.read_csv(TEST_PATH, sep='\\t')\n","    train, val = train_test_split(train, test_size=0.1, random_state=42)\n","    return {\"train\": train, \"val\": val, \"test\": test}\n"]},{"cell_type":"code","execution_count":233,"metadata":{"trusted":true},"outputs":[],"source":["dataset = load_data()"]},{"cell_type":"code","execution_count":234,"metadata":{},"outputs":[],"source":["def encode_labels(examples):\n","            label_dict = {\"entailment\": 0, \"neutral\": 1, \"contradiction\": 2}\n","            # Replace labels in the examples with encoded labels\n","            print(np.unique(examples[\"label\"]))\n","            examples[\"label\"] = [\n","                label_dict[label] for label in examples[\"label\"]\n","            ].copy()\n","            return examples"]},{"cell_type":"code","execution_count":235,"metadata":{"trusted":true},"outputs":[],"source":["train_dataset = dataset[\"train\"]\n","train_dataset, test_dataset = train_test_split(\n","    train_dataset, test_size=0.2, random_state=42\n",")\n","train_dataset, val_dataset = train_test_split(\n","    train_dataset, test_size=0.1, random_state=42\n",")"]},{"cell_type":"code","execution_count":236,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["['contradiction' 'entailment' 'neutral']\n","['contradiction' 'entailment' 'neutral']\n","['contradiction' 'entailment' 'neutral']\n","Dataset({\n","    features: ['pair_id', 'premise', 'hypothesis', 'annotation_1', 'comment_1', 'annotator1_id', 'annotation_2', 'comment_2', 'annotator2_id', 'annotation_3', 'comment_3', 'annotator3_id', 'annotation_FINAL', 'label', '__index_level_0__'],\n","    num_rows: 2844\n","})\n","Dataset({\n","    features: ['pair_id', 'premise', 'hypothesis', 'annotation_1', 'comment_1', 'annotator1_id', 'annotation_2', 'comment_2', 'annotator2_id', 'annotation_3', 'comment_3', 'annotator3_id', 'annotation_FINAL', 'label', '__index_level_0__'],\n","    num_rows: 317\n","})\n","Dataset({\n","    features: ['pair_id', 'premise', 'hypothesis', 'annotation_1', 'comment_1', 'annotator1_id', 'annotation_2', 'comment_2', 'annotator2_id', 'annotation_3', 'comment_3', 'annotator3_id', 'annotation_FINAL', 'label', '__index_level_0__'],\n","    num_rows: 791\n","})\n"]}],"source":["train_dataset = Dataset.from_pandas(encode_labels(train_dataset))\n","val_dataset = Dataset.from_pandas(encode_labels(val_dataset))\n","test_dataset = Dataset.from_pandas(encode_labels(test_dataset))\n","\n","\n","print(train_dataset)\n","print(val_dataset)\n","print(test_dataset)"]},{"cell_type":"code","execution_count":237,"metadata":{"trusted":true},"outputs":[],"source":["# For reference\n","models = [\"EMBEDDIA/sloberta\", \"bert-base-multilingual-cased\"]"]},{"cell_type":"code","execution_count":238,"metadata":{"trusted":true},"outputs":[],"source":["def preprocess_function(examples, model_name, test=False):\n","    tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n","    tokenized = tokenizer(\n","                examples[\"premise\"],\n","                examples[\"hypothesis\"],\n","                padding=\"max_length\",\n","                truncation=True,\n","                max_length=512,\n","            )\n","    if not test:\n","        tokenized[\"label\"] = examples[\"label\"]\n","    return tokenized"]},{"cell_type":"code","execution_count":239,"metadata":{},"outputs":[],"source":["def compute_metrics(eval_pred):\n","    logits, labels = eval_pred\n","    predictions = np.argmax(logits, axis=-1)\n","    return {\n","        'accuracy': accuracy_score(labels, predictions),\n","        'f1': f1_score(labels, predictions, average='macro'),\n","        'precision': precision_score(labels, predictions, average='macro'),\n","        'recall': recall_score(labels, predictions, average='macro')\n","    }\n"]},{"cell_type":"code","execution_count":240,"metadata":{"trusted":true},"outputs":[],"source":["def fine_tune_model(model_name, dataset, model, training_args):\n","    \n","    tokenized_train_dataset = train_dataset.map(\n","        lambda examples: preprocess_function(examples, model_name),\n","        batched=True,\n","    )\n","    tokenized_val_dataset = val_dataset.map(\n","        lambda examples: preprocess_function(examples, model_name),\n","        batched=True,\n","    )\n","    tokenized_test_dataset = test_dataset.map(\n","        lambda examples: preprocess_function(examples, model_name, test=True),\n","        batched=True,\n","    )\n","\n","\n","    data_collator = DataCollatorWithPadding(\n","        tokenizer=AutoTokenizer.from_pretrained(model_name),\n","        padding=\"max_length\",\n","    )\n","\n","    trainer = Trainer(\n","        model=model,\n","        args=training_args,\n","        train_dataset=tokenized_train_dataset,\n","        eval_dataset=tokenized_val_dataset,\n","        data_collator=data_collator,\n","        compute_metrics=compute_metrics,\n","    )\n","\n","    start = time.time()\n","    trainer.train()\n","    elapsed_training = time.time() - start\n","\n","    metrics = trainer.evaluate(tokenized_test_dataset)\n","\n","    print(f\"model: {model_name}, Dataset: SI_NLI, Test Metrics: {metrics}\")\n","\n","    model.save_pretrained(f\"models/{model_name}si_nli\")\n","\n","    return model, metrics, elapsed_training"]},{"cell_type":"code","execution_count":241,"metadata":{"trusted":true},"outputs":[],"source":["def run_lora_sloberta():\n","    model_name = \"EMBEDDIA/sloberta\"\n","    task_type = TaskType.SEQ_CLS\n","    training_args = TrainingArguments(\n","        output_dir=f\"{model_name}-sentinews\",\n","        learning_rate=1e-4,\n","        per_device_train_batch_size=24,\n","        per_device_eval_batch_size=24,\n","        num_train_epochs=3,\n","        weight_decay=0.01,\n","        evaluation_strategy=\"epoch\",\n","        save_strategy=\"epoch\",\n","        load_best_model_at_end=True,\n","    )\n","    model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=3)\n","    model = prepare_model_for_kbit_training(model, task_type)\n","\n","    target_modules = (\n","        [\n","            \"roberta.encoder.layer.\" + str(i) + \".attention.self.query\"\n","            for i in range(model.config.num_hidden_layers)\n","        ]\n","        + [\n","            \"roberta.encoder.layer.\" + str(i) + \".attention.self.key\"\n","            for i in range(model.config.num_hidden_layers)\n","        ]\n","        + [\n","            \"roberta.encoder.layer.\" + str(i) + \".attention.self.value\"\n","            for i in range(model.config.num_hidden_layers)\n","        ]\n","        + [\n","            \"roberta.encoder.layer.\" + str(i) + \".attention.output.dense\"\n","            for i in range(model.config.num_hidden_layers)\n","        ]\n","    )\n","\n","    lora_config = LoraConfig(\n","        r=16,\n","        lora_alpha=32,\n","        lora_dropout=0.05,\n","        task_type=task_type,\n","        bias=\"none\",\n","        target_modules=target_modules,\n","    )\n","\n","    model = get_peft_model(model, lora_config)\n","\n","    _, metrics, elapsed_training = fine_tune_model(\n","        model_name, dataset, model, training_args\n","    )\n","\n","    current_time = time.strftime(\"%Y-%m-%d-%H-%M-%S\")\n","    with open(\"results.csv\", \"a\") as f:\n","        f.write(\n","            f\"{current_time},{model_name},Sentinews, {metrics},{elapsed_training}\\n\"\n","        )"]},{"cell_type":"code","execution_count":242,"metadata":{},"outputs":[],"source":["def run_prefix_tune_sloberta():\n","    model_name = \"EMBEDDIA/sloberta\"\n","    task_type = TaskType.SEQ_CLS\n","    training_args = TrainingArguments(\n","        output_dir=f\"{model_name}-sentinews\",\n","        learning_rate=1e-4,\n","        per_device_train_batch_size=24,\n","        per_device_eval_batch_size=24,\n","        num_train_epochs=3,\n","        weight_decay=0.01,\n","        evaluation_strategy=\"epoch\",\n","        save_strategy=\"epoch\",\n","        load_best_model_at_end=True,\n","    )\n","    model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=3)\n","    model = prepare_model_for_kbit_training(model, task_type)\n","\n","    target_modules = (\n","        [\n","            \"roberta.encoder.layer.\" + str(i) + \".attention.self.query\"\n","            for i in range(model.config.num_hidden_layers)\n","        ]\n","        + [\n","            \"roberta.encoder.layer.\" + str(i) + \".attention.self.key\"\n","            for i in range(model.config.num_hidden_layers)\n","        ]\n","        + [\n","            \"roberta.encoder.layer.\" + str(i) + \".attention.self.value\"\n","            for i in range(model.config.num_hidden_layers)\n","        ]\n","        + [\n","            \"roberta.encoder.layer.\" + str(i) + \".attention.output.dense\"\n","            for i in range(model.config.num_hidden_layers)\n","        ]\n","    )\n","\n","    prefix_config = PrefixTuningConfig(task_type=\"SEQ_CLS\", num_virtual_tokens=20)\n","\n","    model = get_peft_model(model, prefix_config)\n","\n","    _, metrics, elapsed_training = fine_tune_model(\n","        model_name, dataset, model, training_args\n","    )\n","\n","    current_time = time.strftime(\"%Y-%m-%d-%H-%M-%S\")\n","    with open(\"results.csv\", \"a\") as f:\n","        f.write(\n","            f\"{current_time},{model_name},Sentinews, {metrics},{elapsed_training}\\n\"\n","        )"]},{"cell_type":"code","execution_count":243,"metadata":{},"outputs":[],"source":["def run_ia3_sloberta():\n","    model_name = \"EMBEDDIA/sloberta\"\n","    task_type = TaskType.SEQ_CLS\n","    training_args = TrainingArguments(\n","        output_dir=f\"{model_name}-sentinews\",\n","        learning_rate=1e-4,\n","        per_device_train_batch_size=24,\n","        per_device_eval_batch_size=24,\n","        num_train_epochs=3,\n","        weight_decay=0.01,\n","        evaluation_strategy=\"epoch\",\n","        save_strategy=\"epoch\",\n","        load_best_model_at_end=True,\n","    )\n","    model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=3)\n","    model = prepare_model_for_kbit_training(model, task_type)\n","\n","    feed_forward_modules = [\n","        \"roberta.encoder.layer.\" + str(i) + \".intermediate.dense\"\n","        for i in range(model.config.num_hidden_layers)\n","    ]\n","\n","    target_modules = (\n","        [\n","            \"roberta.encoder.layer.\" + str(i) + \".attention.self.query\"\n","            for i in range(model.config.num_hidden_layers)\n","        ]\n","        + [\n","            \"roberta.encoder.layer.\" + str(i) + \".attention.self.key\"\n","            for i in range(model.config.num_hidden_layers)\n","        ]\n","        + [\n","            \"roberta.encoder.layer.\" + str(i) + \".attention.self.value\"\n","            for i in range(model.config.num_hidden_layers)\n","        ]\n","        + [\n","            \"roberta.encoder.layer.\" + str(i) + \".attention.output.dense\"\n","            for i in range(model.config.num_hidden_layers)\n","        ]\n","        + feed_forward_modules\n","    )\n","\n","    ia3_config = IA3Config(task_type=task_type, feedforward_modules = feed_forward_modules, target_modules=target_modules)\n","\n","    model = get_peft_model(model, ia3_config)\n","\n","    _, metrics, elapsed_training = fine_tune_model(\n","        model_name, dataset, model, training_args\n","    )\n","\n","    current_time = time.strftime(\"%Y-%m-%d-%H-%M-%S\")\n","    with open(\"results.csv\", \"a\") as f:\n","        f.write(\n","            f\"{current_time},{model_name},Sentinews, {metrics},{elapsed_training}\\n\"\n","        )"]},{"cell_type":"code","execution_count":244,"metadata":{"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of CamembertForSequenceClassification were not initialized from the model checkpoint at EMBEDDIA/sloberta and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Map: 100%|██████████| 2844/2844 [00:01<00:00, 2205.10 examples/s]\n","Map: 100%|██████████| 317/317 [00:00<00:00, 906.67 examples/s]\n","Map: 100%|██████████| 791/791 [00:00<00:00, 2163.68 examples/s]\n"]},{"ename":"OutOfMemoryError","evalue":"CUDA out of memory. Tried to allocate 94.00 MiB. GPU ","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)","Cell \u001b[0;32mIn[244], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mrun_ia3_sloberta\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[243], line 47\u001b[0m, in \u001b[0;36mrun_ia3_sloberta\u001b[0;34m()\u001b[0m\n\u001b[1;32m     43\u001b[0m ia3_config \u001b[38;5;241m=\u001b[39m IA3Config(task_type\u001b[38;5;241m=\u001b[39mtask_type, feedforward_modules \u001b[38;5;241m=\u001b[39m feed_forward_modules, target_modules\u001b[38;5;241m=\u001b[39mtarget_modules)\n\u001b[1;32m     45\u001b[0m model \u001b[38;5;241m=\u001b[39m get_peft_model(model, ia3_config)\n\u001b[0;32m---> 47\u001b[0m _, metrics, elapsed_training \u001b[38;5;241m=\u001b[39m \u001b[43mfine_tune_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining_args\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m current_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mstrftime(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY-\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm-\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mH-\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mM-\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mS\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresults.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n","Cell \u001b[0;32mIn[240], line 22\u001b[0m, in \u001b[0;36mfine_tune_model\u001b[0;34m(model_name, dataset, model, training_args)\u001b[0m\n\u001b[1;32m     11\u001b[0m tokenized_test_dataset \u001b[38;5;241m=\u001b[39m test_dataset\u001b[38;5;241m.\u001b[39mmap(\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28;01mlambda\u001b[39;00m examples: preprocess_function(examples, model_name, test\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[1;32m     13\u001b[0m     batched\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     14\u001b[0m )\n\u001b[1;32m     17\u001b[0m data_collator \u001b[38;5;241m=\u001b[39m DataCollatorWithPadding(\n\u001b[1;32m     18\u001b[0m     tokenizer\u001b[38;5;241m=\u001b[39mAutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_name),\n\u001b[1;32m     19\u001b[0m     padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_length\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     20\u001b[0m )\n\u001b[0;32m---> 22\u001b[0m trainer \u001b[38;5;241m=\u001b[39m \u001b[43mTrainer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenized_train_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenized_val_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_collator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_collator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompute_metrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompute_metrics\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m     32\u001b[0m trainer\u001b[38;5;241m.\u001b[39mtrain()\n","File \u001b[0;32m~/anaconda3/envs/benchmark/lib/python3.10/site-packages/transformers/trainer.py:514\u001b[0m, in \u001b[0;36mTrainer.__init__\u001b[0;34m(self, model, args, data_collator, train_dataset, eval_dataset, tokenizer, model_init, compute_metrics, callbacks, optimizers, preprocess_logits_for_metrics)\u001b[0m\n\u001b[1;32m    509\u001b[0m \u001b[38;5;66;03m# Bnb Quantized models doesn't support `.to` operation.\u001b[39;00m\n\u001b[1;32m    510\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    511\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mplace_model_on_device\n\u001b[1;32m    512\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(model, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquantization_method\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m QuantizationMethod\u001b[38;5;241m.\u001b[39mBITS_AND_BYTES\n\u001b[1;32m    513\u001b[0m ):\n\u001b[0;32m--> 514\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_move_model_to_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    516\u001b[0m \u001b[38;5;66;03m# Force n_gpu to 1 to avoid DataParallel as MP will manage the GPUs\u001b[39;00m\n\u001b[1;32m    517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_model_parallel:\n","File \u001b[0;32m~/anaconda3/envs/benchmark/lib/python3.10/site-packages/transformers/trainer.py:757\u001b[0m, in \u001b[0;36mTrainer._move_model_to_device\u001b[0;34m(self, model, device)\u001b[0m\n\u001b[1;32m    756\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_move_model_to_device\u001b[39m(\u001b[38;5;28mself\u001b[39m, model, device):\n\u001b[0;32m--> 757\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    758\u001b[0m     \u001b[38;5;66;03m# Moving a model to an XLA device disconnects the tied weights, so we have to retie them.\u001b[39;00m\n\u001b[1;32m    759\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mparallel_mode \u001b[38;5;241m==\u001b[39m ParallelMode\u001b[38;5;241m.\u001b[39mTPU \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(model, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtie_weights\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n","File \u001b[0;32m~/anaconda3/envs/benchmark/lib/python3.10/site-packages/torch/nn/modules/module.py:1173\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1170\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1171\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m-> 1173\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/anaconda3/envs/benchmark/lib/python3.10/site-packages/torch/nn/modules/module.py:779\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    777\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    778\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 779\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    781\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    782\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    783\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    784\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    789\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    790\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n","File \u001b[0;32m~/anaconda3/envs/benchmark/lib/python3.10/site-packages/torch/nn/modules/module.py:779\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    777\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    778\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 779\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    781\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    782\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    783\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    784\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    789\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    790\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n","    \u001b[0;31m[... skipping similar frames: Module._apply at line 779 (2 times)]\u001b[0m\n","File \u001b[0;32m~/anaconda3/envs/benchmark/lib/python3.10/site-packages/torch/nn/modules/module.py:779\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    777\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    778\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 779\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    781\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    782\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    783\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    784\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    789\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    790\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n","File \u001b[0;32m~/anaconda3/envs/benchmark/lib/python3.10/site-packages/torch/nn/modules/module.py:804\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    800\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    801\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    802\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    803\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 804\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    805\u001b[0m p_should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    807\u001b[0m \u001b[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n","File \u001b[0;32m~/anaconda3/envs/benchmark/lib/python3.10/site-packages/torch/nn/modules/module.py:1159\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1152\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m   1153\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(\n\u001b[1;32m   1154\u001b[0m             device,\n\u001b[1;32m   1155\u001b[0m             dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1156\u001b[0m             non_blocking,\n\u001b[1;32m   1157\u001b[0m             memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format,\n\u001b[1;32m   1158\u001b[0m         )\n\u001b[0;32m-> 1159\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1160\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1161\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1162\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1163\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1164\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1165\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot copy out of meta tensor; no data!\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n","\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 94.00 MiB. GPU "]}],"source":["run_lora_sloberta()"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30699,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"}},"nbformat":4,"nbformat_minor":4}
