{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-02T18:59:27.237653Z","iopub.status.busy":"2024-05-02T18:59:27.237237Z","iopub.status.idle":"2024-05-02T18:59:46.674630Z","shell.execute_reply":"2024-05-02T18:59:46.673650Z","shell.execute_reply.started":"2024-05-02T18:59:27.237615Z"},"trusted":true},"outputs":[],"source":["import wandb\n","WANDB_API = \"\"\n","wandb.login(key=WANDB_API)\n","\n","wandb.init(\n","    # set the wandb project where this run will be logged\n","    project=\"peft\",\n","\n","    # track hyperparameters and run metadata\n","    config={\n","    \"architecture\": \"BERT\",\n","    \"dataset\": \"ssj500k\",\n","    \"epochs\": 10,\n","    \"learning_rate\":2e-5,\n","    \"per_device_train_batch_size\":16,\n","    \"per_device_eval_batch_size\":16,\n","    \"num_train_epochs\":2,\n","    \"weight_decay\":0.01,\n","    }\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-02T18:59:46.677224Z","iopub.status.busy":"2024-05-02T18:59:46.676573Z","iopub.status.idle":"2024-05-02T19:00:11.766238Z","shell.execute_reply":"2024-05-02T19:00:11.765021Z","shell.execute_reply.started":"2024-05-02T18:59:46.677189Z"},"trusted":true},"outputs":[],"source":["!pip install seqeval\n","!pip install evaluate"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-02T19:00:11.768060Z","iopub.status.busy":"2024-05-02T19:00:11.767726Z","iopub.status.idle":"2024-05-02T19:00:43.349306Z","shell.execute_reply":"2024-05-02T19:00:43.348240Z","shell.execute_reply.started":"2024-05-02T19:00:11.768030Z"},"trusted":true},"outputs":[],"source":["!pip install -q peft\n","import torch\n","import time\n","import os\n","from transformers import (\n","    AutoModelForTokenClassification,\n","    AutoTokenizer,\n","    DataCollatorWithPadding,\n","    Trainer,\n","    TrainingArguments,\n",")\n","from sklearn.model_selection import train_test_split\n","from datasets import Dataset\n","from datasets import load_dataset, load_metric\n","from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, TaskType, PrefixTuningConfig, IA3Config, PeftModel\n","from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score\n","import pandas as pd\n","import numpy as np\n","import evaluate\n","seqeval = evaluate.load(\"seqeval\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-02T19:00:43.354370Z","iopub.status.busy":"2024-05-02T19:00:43.353266Z","iopub.status.idle":"2024-05-02T19:00:46.364696Z","shell.execute_reply":"2024-05-02T19:00:46.363691Z","shell.execute_reply.started":"2024-05-02T19:00:43.354331Z"},"trusted":true},"outputs":[],"source":["dataset = load_dataset(\"cjvt/ssj500k\", \"named_entity_recognition\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-02T19:03:10.682440Z","iopub.status.busy":"2024-05-02T19:03:10.681552Z","iopub.status.idle":"2024-05-02T19:03:10.854665Z","shell.execute_reply":"2024-05-02T19:03:10.853615Z","shell.execute_reply.started":"2024-05-02T19:03:10.682403Z"},"trusted":true},"outputs":[],"source":["train_dataset = dataset[\"train\"].to_pandas()\n","train_dataset, test_dataset = train_test_split(\n","    train_dataset, test_size=0.2, random_state=42\n",")\n","train_dataset, val_dataset = train_test_split(\n","    train_dataset, test_size=0.1, random_state=42\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-02T19:03:12.740029Z","iopub.status.busy":"2024-05-02T19:03:12.739031Z","iopub.status.idle":"2024-05-02T19:03:12.753809Z","shell.execute_reply":"2024-05-02T19:03:12.752831Z","shell.execute_reply.started":"2024-05-02T19:03:12.739985Z"},"trusted":true},"outputs":[],"source":["id2label = {0: 'O',\n"," 1: 'B-LOC',\n"," 2: 'I-LOC',\n"," 3: 'B-ORG',\n"," 4: 'I-ORG',\n"," 5: 'B-PER',\n"," 6: 'I-PER',\n"," 7: 'B-MISC',\n"," 8: 'I-MISC'\n","}\n","\n","label2id = {label: id for id,label in id2label.items()}\n","\n","display(label2id)\n","display(id2label)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-02T19:03:16.208366Z","iopub.status.busy":"2024-05-02T19:03:16.207690Z","iopub.status.idle":"2024-05-02T19:03:16.497879Z","shell.execute_reply":"2024-05-02T19:03:16.497032Z","shell.execute_reply.started":"2024-05-02T19:03:16.208335Z"},"trusted":true},"outputs":[],"source":["train_dataset = Dataset.from_pandas(train_dataset)\n","val_dataset = Dataset.from_pandas(val_dataset)\n","test_dataset = Dataset.from_pandas(test_dataset)\n","\n","\n","print(train_dataset)\n","print(val_dataset)\n","print(test_dataset)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-02T19:03:18.462005Z","iopub.status.busy":"2024-05-02T19:03:18.461302Z","iopub.status.idle":"2024-05-02T19:03:18.466890Z","shell.execute_reply":"2024-05-02T19:03:18.465925Z","shell.execute_reply.started":"2024-05-02T19:03:18.461958Z"},"trusted":true},"outputs":[],"source":["# For reference\n","models = [\"EMBEDDIA/sloberta\", \"bert-base-multilingual-cased\"]\n","model_name = models[0]"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-02T19:03:20.715670Z","iopub.status.busy":"2024-05-02T19:03:20.714811Z","iopub.status.idle":"2024-05-02T19:03:21.782781Z","shell.execute_reply":"2024-05-02T19:03:21.781652Z","shell.execute_reply.started":"2024-05-02T19:03:20.715638Z"},"trusted":true},"outputs":[],"source":["tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n","from transformers import DataCollatorForTokenClassification\n","data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-02T19:03:24.199051Z","iopub.status.busy":"2024-05-02T19:03:24.198313Z","iopub.status.idle":"2024-05-02T19:03:24.208247Z","shell.execute_reply":"2024-05-02T19:03:24.207048Z","shell.execute_reply.started":"2024-05-02T19:03:24.199019Z"},"trusted":true},"outputs":[],"source":["def tokenize_and_align_labels(examples, tokenizer):\n","    tokenized_inputs = tokenizer(examples[\"words\"], truncation=True, is_split_into_words=True)\n","    \n","    labels = []\n","    for i, label in enumerate(examples[f\"ne_tags\"]):\n","        word_ids = tokenized_inputs.word_ids(batch_index=i)  # Map tokens to their respective word.\n","        previous_word_idx = None\n","        label_ids = []\n","        for word_idx in word_ids:  # Set the special tokens to -100.\n","            if word_idx is None:\n","                label_ids.append(-100)\n","            elif word_idx != previous_word_idx:  # Only label the first token of a given word.\n","                label_ids.append(label2id[label[word_idx]])\n","            else:\n","                label_ids.append(-100)\n","            previous_word_idx = word_idx\n","        labels.append(label_ids)\n","\n","    tokenized_inputs[\"labels\"] = labels\n","    return tokenized_inputs"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-02T19:03:27.801725Z","iopub.status.busy":"2024-05-02T19:03:27.800831Z","iopub.status.idle":"2024-05-02T19:03:27.808857Z","shell.execute_reply":"2024-05-02T19:03:27.806860Z","shell.execute_reply.started":"2024-05-02T19:03:27.801694Z"},"trusted":true},"outputs":[],"source":["def preprocess_function(examples, tokenizer):\n","    tokenized_inputs = tokenize_and_align_labels(examples, tokenizer)\n","    return tokenized_inputs"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-02T19:03:29.657528Z","iopub.status.busy":"2024-05-02T19:03:29.657157Z","iopub.status.idle":"2024-05-02T19:03:31.211254Z","shell.execute_reply":"2024-05-02T19:03:31.210223Z","shell.execute_reply.started":"2024-05-02T19:03:29.657500Z"},"trusted":true},"outputs":[],"source":["tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n","tokenized_train = train_dataset.map(lambda example: preprocess_function(example, tokenizer), batched=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-02T19:07:44.953475Z","iopub.status.busy":"2024-05-02T19:07:44.952590Z","iopub.status.idle":"2024-05-02T19:07:44.964064Z","shell.execute_reply":"2024-05-02T19:07:44.962446Z","shell.execute_reply.started":"2024-05-02T19:07:44.953441Z"},"trusted":true},"outputs":[],"source":["def compute_metrics(p):\n","    predictions, labels = p\n","    predictions = np.argmax(predictions, axis=2)\n","\n","    true_predictions = [\n","        [id2label[p] for (p, l) in zip(prediction, label) if l != -100]\n","        for prediction, label in zip(predictions, labels)\n","    ]\n","    true_labels = [\n","        [id2label[l] for (p, l) in zip(prediction, label) if l != -100]\n","        for prediction, label in zip(predictions, labels)\n","    ]\n","\n","    results = seqeval.compute(predictions=true_predictions, references=true_labels)\n","    result_metrics = {\n","        \"precision\": results[\"overall_precision\"],\n","        \"recall\": results[\"overall_recall\"],\n","        \"f1\": results[\"overall_f1\"],\n","        \"accuracy\": results[\"overall_accuracy\"],\n","    }\n","    wandb.log(result_metrics)\n","    return result_metrics"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-05-02T21:19:17.163863Z","iopub.status.idle":"2024-05-02T21:19:17.164404Z","shell.execute_reply":"2024-05-02T21:19:17.164148Z","shell.execute_reply.started":"2024-05-02T21:19:17.164125Z"},"trusted":true},"outputs":[],"source":["def fine_tune_model(model_name, dataset, model, training_args):\n","    \n","    tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n","    print(dataset)\n","    \n","    tokenized_train_dataset = dataset[\"train\"].map(\n","        lambda examples: preprocess_function(examples, tokenizer),\n","        batched=True,\n","    )\n","    tokenized_val_dataset = dataset[\"val\"].map(\n","        lambda examples: preprocess_function(examples, tokenizer),\n","        batched=True,\n","    )\n","    tokenized_test_dataset = dataset[\"test\"].map(\n","        lambda examples: preprocess_function(examples, tokenizer),\n","        batched=True,\n","    )\n","\n","    data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n","\n","    trainer = Trainer(\n","        model=model,\n","        args=training_args,\n","        train_dataset=tokenized_train_dataset,\n","        eval_dataset=tokenized_val_dataset,\n","        data_collator=data_collator,\n","        compute_metrics=compute_metrics,\n","    )\n","\n","    start = time.time()\n","    trainer.train()\n","    elapsed_training = time.time() - start\n","\n","    metrics = trainer.evaluate(tokenized_test_dataset)\n","\n","    print(f\"model: {model_name}, Dataset: ssj500k, Test Metrics: {metrics}\")\n","\n","    model.save_pretrained(f\"models/{model_name}_ner_sj500k\")\n","\n","    return model, metrics, elapsed_training"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["model_name = models[0]\n","model = AutoModelForTokenClassification.from_pretrained(\n","    model_name, num_labels=9, id2label=id2label, label2id=label2id\n",")\n","\n","training_args = TrainingArguments(\n","    output_dir=f\"ner_fully_finetuned_{model_name}\",\n","    learning_rate=2e-5,\n","    per_device_train_batch_size=16,\n","    per_device_eval_batch_size=16,\n","    num_train_epochs=2,\n","    weight_decay=0.01,\n","    evaluation_strategy=\"epoch\",\n","    save_strategy=\"epoch\",\n","    load_best_model_at_end=True,\n",")\n","\n","wandb.config[\"base_model\"] = model_name\n","\n","dataset = {\"train\": train_dataset, \"val\": val_dataset, \"test\": test_dataset}\n","model, metrics, elapsed_training =  fine_tune_model(model_name, dataset, model, training_args=training_args)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-02T21:29:07.053433Z","iopub.status.busy":"2024-05-02T21:29:07.052948Z","iopub.status.idle":"2024-05-02T21:32:17.184960Z","shell.execute_reply":"2024-05-02T21:32:17.183667Z","shell.execute_reply.started":"2024-05-02T21:29:07.053400Z"},"trusted":true},"outputs":[],"source":["def fully_finetune_sloberta():\n","    model_name = \"EMBEDDIA/sloberta\"\n","    model = AutoModelForTokenClassification.from_pretrained(\n","        model_name, num_labels=9, id2label=id2label, label2id=label2id\n","    )\n","\n","    training_args = TrainingArguments(\n","        output_dir=f\"ner_fully_finetuned_{model_name}\",\n","        learning_rate=2e-5,\n","        per_device_train_batch_size=16,\n","        per_device_eval_batch_size=16,\n","        num_train_epochs=3,\n","        weight_decay=0.01,\n","        evaluation_strategy=\"epoch\",\n","        save_strategy=\"epoch\",\n","        load_best_model_at_end=True,\n","    )\n","\n","    wandb.config[\"base_model\"] = model_name\n","\n","    dataset = {\"train\": train_dataset, \"val\": val_dataset, \"test\": test_dataset}\n","    model, metrics, elapsed_training =  fine_tune_model(model_name, dataset, model, training_args=training_args)\n","    current_time = time.strftime(\"%Y-%m-%d-%H-%M-%S\")\n","    with open(\"results.csv\", \"a\") as f:\n","        f.write(\n","            f\"{current_time},{model_name},SSJ500-NER, {metrics},{elapsed_training}\\n\"\n","        )\n","    print(f\"Training time: {elapsed_training}\")\n","\n","fully_finetune_sloberta()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-02T23:52:36.815353Z","iopub.status.busy":"2024-05-02T23:52:36.814442Z","iopub.status.idle":"2024-05-02T23:56:59.265842Z","shell.execute_reply":"2024-05-02T23:56:59.264807Z","shell.execute_reply.started":"2024-05-02T23:52:36.815320Z"},"trusted":true},"outputs":[],"source":["def fully_finetune_bert():\n","    model_name = \"bert-base-multilingual-cased\"\n","    model = AutoModelForTokenClassification.from_pretrained(\n","        model_name, num_labels=9, id2label=id2label, label2id=label2id\n","    )\n","\n","    training_args = TrainingArguments(\n","        output_dir=f\"ner_fully_finetuned_{model_name}\",\n","        learning_rate=2e-5,\n","        per_device_train_batch_size=16,\n","        per_device_eval_batch_size=16,\n","        num_train_epochs=3,\n","        weight_decay=0.01,\n","        evaluation_strategy=\"epoch\",\n","        save_strategy=\"epoch\",\n","        load_best_model_at_end=True,\n","    )\n","\n","    wandb.config[\"base_model\"] = model_name\n","\n","    dataset = {\"train\": train_dataset, \"val\": val_dataset, \"test\": test_dataset}\n","    model, metrics, elapsed_training =  fine_tune_model(model_name, dataset, model, training_args=training_args)\n","    current_time = time.strftime(\"%Y-%m-%d-%H-%M-%S\")\n","    with open(\"results.csv\", \"a\") as f:\n","        f.write(\n","            f\"{current_time},{model_name},SSJ500-NER, {metrics},{elapsed_training}\\n\"\n","        )\n","    print(f\"Training time: {elapsed_training}\")\n","\n","fully_finetune_bert()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-02T21:44:22.114176Z","iopub.status.busy":"2024-05-02T21:44:22.113444Z","iopub.status.idle":"2024-05-02T21:44:22.133132Z","shell.execute_reply":"2024-05-02T21:44:22.132193Z","shell.execute_reply.started":"2024-05-02T21:44:22.114144Z"},"trusted":true},"outputs":[],"source":["def run_lora_sloberta(dataset):\n","    model_name = \"EMBEDDIA/sloberta\"\n","    task_type = TaskType.TOKEN_CLS\n","    training_args = TrainingArguments(\n","        output_dir=f\"ner_lora_finetuned_{model_name}\",\n","        learning_rate=1e-3,\n","        per_device_train_batch_size=24,\n","        per_device_eval_batch_size=24,\n","        num_train_epochs=3,\n","        weight_decay=0.01,\n","        evaluation_strategy=\"epoch\",\n","        save_strategy=\"epoch\",\n","        load_best_model_at_end=True,\n","    )\n","    model = AutoModelForTokenClassification.from_pretrained(\n","        model_name, num_labels=9, id2label=id2label, label2id=label2id\n","    )\n","\n","    target_modules = (\n","        [\n","            \"roberta.encoder.layer.\" + str(i) + \".attention.self.query\"\n","            for i in range(model.config.num_hidden_layers)\n","        ]\n","        + [\n","            \"roberta.encoder.layer.\" + str(i) + \".attention.self.key\"\n","            for i in range(model.config.num_hidden_layers)\n","        ]\n","        + [\n","            \"roberta.encoder.layer.\" + str(i) + \".attention.self.value\"\n","            for i in range(model.config.num_hidden_layers)\n","        ]\n","        + [\n","            \"roberta.encoder.layer.\" + str(i) + \".attention.output.dense\"\n","            for i in range(model.config.num_hidden_layers)\n","        ]\n","    )\n","    \n","    peft_config = LoraConfig(\n","        task_type=TaskType.TOKEN_CLS, inference_mode=False, r=16, lora_alpha=16, lora_dropout=0.1, bias=\"all\"\n","    )\n","\n","    lora_config = LoraConfig(\n","        r=16,\n","        lora_alpha=32,\n","        lora_dropout=0.05,\n","        task_type=task_type,\n","        bias=\"none\",\n","        target_modules=target_modules,\n","    )\n","\n","    model = get_peft_model(model, lora_config)\n","\n","    _, metrics, elapsed_training = fine_tune_model(\n","        model_name, dataset, model, training_args\n","    )\n","    print(f\"Training time: {elapsed_training}\")\n","    current_time = time.strftime(\"%Y-%m-%d-%H-%M-%S\")\n","    with open(\"results.csv\", \"a\") as f:\n","        f.write(\n","            f\"{current_time},{model_name},SSJ500-NER, {metrics},{elapsed_training}\\n\"\n","        )\n","    print(f\"Training time: {elapsed_training}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-02T20:22:05.553651Z","iopub.status.busy":"2024-05-02T20:22:05.553274Z","iopub.status.idle":"2024-05-02T20:24:44.674912Z","shell.execute_reply":"2024-05-02T20:24:44.673770Z","shell.execute_reply.started":"2024-05-02T20:22:05.553622Z"},"trusted":true},"outputs":[],"source":["run_lora_sloberta(dataset = {\"train\": train_dataset, \"val\": val_dataset, \"test\": test_dataset})"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-03T00:13:39.130135Z","iopub.status.busy":"2024-05-03T00:13:39.129376Z","iopub.status.idle":"2024-05-03T00:13:39.143805Z","shell.execute_reply":"2024-05-03T00:13:39.142737Z","shell.execute_reply.started":"2024-05-03T00:13:39.130102Z"},"trusted":true},"outputs":[],"source":["def run_lora_bert(dataset):\n","    model_name = \"bert-base-multilingual-cased\"\n","    task_type = TaskType.TOKEN_CLS\n","    training_args = TrainingArguments(\n","        output_dir=f\"ner_lora_finetuned_{model_name}\",\n","        learning_rate=1e-3,\n","        per_device_train_batch_size=16,\n","        per_device_eval_batch_size=16,\n","        num_train_epochs=3,\n","        weight_decay=0.01,\n","        evaluation_strategy=\"epoch\",\n","        save_strategy=\"epoch\",\n","        load_best_model_at_end=True,\n","    )\n","    model = AutoModelForTokenClassification.from_pretrained(\n","        model_name, num_labels=9, id2label=id2label, label2id=label2id\n","    )\n","\n","    target_modules = (\n","        [\n","            \"bert.encoder.layer.\" + str(i) + \".attention.self.query\"\n","            for i in range(model.config.num_hidden_layers)\n","        ]\n","        + [\n","            \"bert.encoder.layer.\" + str(i) + \".attention.self.key\"\n","            for i in range(model.config.num_hidden_layers)\n","        ]\n","        + [\n","            \"bert.encoder.layer.\" + str(i) + \".attention.self.value\"\n","            for i in range(model.config.num_hidden_layers)\n","        ]\n","        + [\n","            \"bert.encoder.layer.\" + str(i) + \".attention.output.dense\"\n","            for i in range(model.config.num_hidden_layers)\n","        ]\n","    )\n","    \n","    peft_config = LoraConfig(\n","        task_type=TaskType.TOKEN_CLS, inference_mode=False, r=16, lora_alpha=16, lora_dropout=0.1, bias=\"all\"\n","    )\n","\n","    lora_config = LoraConfig(\n","        r=16,\n","        lora_alpha=32,\n","        lora_dropout=0.05,\n","        task_type=task_type,\n","        bias=\"none\",\n","        target_modules=target_modules,\n","    )\n","\n","    model = get_peft_model(model, lora_config)\n","\n","    _, metrics, elapsed_training = fine_tune_model(\n","        model_name, dataset, model, training_args\n","    )\n","    print(f\"Training time: {elapsed_training}\")\n","    current_time = time.strftime(\"%Y-%m-%d-%H-%M-%S\")\n","    with open(\"results.csv\", \"a\") as f:\n","        f.write(\n","            f\"{current_time},{model_name},SSJ500-NER, {metrics},{elapsed_training}\\n\"\n","        )\n","    print(f\"Training time: {elapsed_training}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-03T00:13:48.264369Z","iopub.status.busy":"2024-05-03T00:13:48.263948Z","iopub.status.idle":"2024-05-03T00:17:04.343895Z","shell.execute_reply":"2024-05-03T00:17:04.342959Z","shell.execute_reply.started":"2024-05-03T00:13:48.264340Z"},"trusted":true},"outputs":[],"source":["run_lora_bert(dataset = {\"train\": train_dataset, \"val\": val_dataset, \"test\": test_dataset})"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-02T22:23:25.817289Z","iopub.status.busy":"2024-05-02T22:23:25.816578Z","iopub.status.idle":"2024-05-02T22:23:25.829406Z","shell.execute_reply":"2024-05-02T22:23:25.828227Z","shell.execute_reply.started":"2024-05-02T22:23:25.817255Z"},"trusted":true},"outputs":[],"source":["def run_prefix_tune_sloberta(dataset):\n","    model_name = \"EMBEDDIA/sloberta\"\n","\n","    task_type = TaskType.TOKEN_CLS\n","    training_args = TrainingArguments(\n","        output_dir=f\"ner_prefix_tunning_finetuned_{model_name}\",\n","        learning_rate=1e-2,\n","        per_device_train_batch_size=16,\n","        per_device_eval_batch_size=16,\n","        num_train_epochs=3,\n","        weight_decay=0.01,\n","        evaluation_strategy=\"epoch\",\n","        save_strategy=\"epoch\",\n","        load_best_model_at_end=True,\n","    )\n","    model = AutoModelForTokenClassification.from_pretrained(\n","        model_name, num_labels=9, id2label=id2label, label2id=label2id\n","    )\n","\n","    target_modules = (\n","        [\n","            \"roberta.encoder.layer.\" + str(i) + \".attention.self.query\"\n","            for i in range(model.config.num_hidden_layers)\n","        ]\n","        + [\n","            \"roberta.encoder.layer.\" + str(i) + \".attention.self.key\"\n","            for i in range(model.config.num_hidden_layers)\n","        ]\n","        + [\n","            \"roberta.encoder.layer.\" + str(i) + \".attention.self.value\"\n","            for i in range(model.config.num_hidden_layers)\n","        ]\n","        + [\n","            \"roberta.encoder.layer.\" + str(i) + \".attention.output.dense\"\n","            for i in range(model.config.num_hidden_layers)\n","        ]\n","    )\n","\n","    prefix_config = PrefixTuningConfig(task_type=\"TOKEN_CLS\", num_virtual_tokens=20)\n","\n","    model = get_peft_model(model, prefix_config)\n","\n","    _, metrics, elapsed_training = fine_tune_model(\n","        model_name, dataset, model, training_args\n","    )\n","    print(f\"Training time: {elapsed_training}\")\n","    current_time = time.strftime(\"%Y-%m-%d-%H-%M-%S\")\n","    with open(\"results.csv\", \"a\") as f:\n","        f.write(\n","            f\"{current_time},{model_name},SSJ500-NER, {metrics},{elapsed_training}\\n\"\n","        )"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-02T22:23:28.986322Z","iopub.status.busy":"2024-05-02T22:23:28.985436Z","iopub.status.idle":"2024-05-02T22:25:35.259769Z","shell.execute_reply":"2024-05-02T22:25:35.258668Z","shell.execute_reply.started":"2024-05-02T22:23:28.986288Z"},"trusted":true},"outputs":[],"source":["run_prefix_tune_sloberta(dataset = {\"train\": train_dataset, \"val\": val_dataset, \"test\": test_dataset})"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-03T00:19:05.522067Z","iopub.status.busy":"2024-05-03T00:19:05.521620Z","iopub.status.idle":"2024-05-03T00:19:05.534420Z","shell.execute_reply":"2024-05-03T00:19:05.533009Z","shell.execute_reply.started":"2024-05-03T00:19:05.522036Z"},"trusted":true},"outputs":[],"source":["def run_prefix_tune_bert(dataset):\n","    model_name = \"bert-base-multilingual-cased\"\n","\n","    task_type = TaskType.TOKEN_CLS\n","    training_args = TrainingArguments(\n","        output_dir=f\"ner_prefix_tunning_finetuned_{model_name}\",\n","        learning_rate=1e-2,\n","        per_device_train_batch_size=16,\n","        per_device_eval_batch_size=16,\n","        num_train_epochs=3,\n","        weight_decay=0.01,\n","        evaluation_strategy=\"epoch\",\n","        save_strategy=\"epoch\",\n","        load_best_model_at_end=True,\n","    )\n","    model = AutoModelForTokenClassification.from_pretrained(\n","        model_name, num_labels=9, id2label=id2label, label2id=label2id\n","    )\n","\n","    target_modules = (\n","        [\n","            \"bert.encoder.layer.\" + str(i) + \".attention.self.query\"\n","            for i in range(model.config.num_hidden_layers)\n","        ]\n","        + [\n","            \"bert.encoder.layer.\" + str(i) + \".attention.self.key\"\n","            for i in range(model.config.num_hidden_layers)\n","        ]\n","        + [\n","            \"bert.encoder.layer.\" + str(i) + \".attention.self.value\"\n","            for i in range(model.config.num_hidden_layers)\n","        ]\n","        + [\n","            \"bert.encoder.layer.\" + str(i) + \".attention.output.dense\"\n","            for i in range(model.config.num_hidden_layers)\n","        ]\n","    )\n","\n","    prefix_config = PrefixTuningConfig(task_type=\"TOKEN_CLS\", num_virtual_tokens=20)\n","\n","    model = get_peft_model(model, prefix_config)\n","\n","    _, metrics, elapsed_training = fine_tune_model(\n","        model_name, dataset, model, training_args\n","    )\n","    print(f\"Training time: {elapsed_training}\")\n","    current_time = time.strftime(\"%Y-%m-%d-%H-%M-%S\")\n","    with open(\"results.csv\", \"a\") as f:\n","        f.write(\n","            f\"{current_time},{model_name},SSJ500-NER, {metrics},{elapsed_training}\\n\"\n","        )"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-03T00:19:10.913557Z","iopub.status.busy":"2024-05-03T00:19:10.912690Z","iopub.status.idle":"2024-05-03T00:21:55.720373Z","shell.execute_reply":"2024-05-03T00:21:55.718994Z","shell.execute_reply.started":"2024-05-03T00:19:10.913525Z"},"trusted":true},"outputs":[],"source":["run_prefix_tune_bert(dataset = {\"train\": train_dataset, \"val\": val_dataset, \"test\": test_dataset})"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-03T00:23:34.316203Z","iopub.status.busy":"2024-05-03T00:23:34.315305Z","iopub.status.idle":"2024-05-03T00:23:34.334468Z","shell.execute_reply":"2024-05-03T00:23:34.333395Z","shell.execute_reply.started":"2024-05-03T00:23:34.316166Z"},"trusted":true},"outputs":[],"source":["def run_ia3_sloberta(dataset):\n","    model_name = \"EMBEDDIA/sloberta\"\n","    task_type = TaskType.TOKEN_CLS\n","    training_args = TrainingArguments(\n","        output_dir=f\"ner-ia3-{model_name}\",\n","        learning_rate=1e-2,\n","        per_device_train_batch_size=16,\n","        per_device_eval_batch_size=16,\n","        num_train_epochs=3,\n","        weight_decay=0.01,\n","        evaluation_strategy=\"epoch\",\n","        save_strategy=\"epoch\",\n","        load_best_model_at_end=True,\n","    )\n","    model = AutoModelForTokenClassification.from_pretrained(\n","        model_name, num_labels=9, id2label=id2label, label2id=label2id\n","    )\n","    model = prepare_model_for_kbit_training(model, task_type)\n","\n","    feed_forward_modules = [\n","        \"roberta.encoder.layer.\" + str(i) + \".intermediate.dense\"\n","        for i in range(model.config.num_hidden_layers)\n","    ]\n","\n","    target_modules = (\n","        [\n","            \"roberta.encoder.layer.\" + str(i) + \".attention.self.query\"\n","            for i in range(model.config.num_hidden_layers)\n","        ]\n","        + [\n","            \"roberta.encoder.layer.\" + str(i) + \".attention.self.key\"\n","            for i in range(model.config.num_hidden_layers)\n","        ]\n","        + [\n","            \"roberta.encoder.layer.\" + str(i) + \".attention.self.value\"\n","            for i in range(model.config.num_hidden_layers)\n","        ]\n","        + [\n","            \"roberta.encoder.layer.\" + str(i) + \".attention.output.dense\"\n","            for i in range(model.config.num_hidden_layers)\n","        ]\n","        + feed_forward_modules\n","    )\n","\n","    ia3_config = IA3Config(task_type=task_type, feedforward_modules = feed_forward_modules, target_modules=target_modules)\n","\n","    model = get_peft_model(model, ia3_config)\n","\n","    _, metrics, elapsed_training = fine_tune_model(\n","        model_name, dataset, model, training_args\n","    )\n","    print(f\"Training time: {elapsed_training}\")\n","    current_time = time.strftime(\"%Y-%m-%d-%H-%M-%S\")\n","    with open(\"results.csv\", \"a\") as f:\n","        f.write(\n","            f\"{current_time},{model_name},Sentinews, {metrics},{elapsed_training}\\n\"\n","        )"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-03T00:23:37.179203Z","iopub.status.busy":"2024-05-03T00:23:37.178845Z","iopub.status.idle":"2024-05-03T00:23:44.831721Z","shell.execute_reply":"2024-05-03T00:23:44.830045Z","shell.execute_reply.started":"2024-05-03T00:23:37.179176Z"},"trusted":true},"outputs":[],"source":["run_ia3_sloberta(dataset = {\"train\": train_dataset, \"val\": val_dataset, \"test\": test_dataset})"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-03T00:43:17.364862Z","iopub.status.busy":"2024-05-03T00:43:17.364141Z","iopub.status.idle":"2024-05-03T00:43:17.376705Z","shell.execute_reply":"2024-05-03T00:43:17.375657Z","shell.execute_reply.started":"2024-05-03T00:43:17.364832Z"},"trusted":true},"outputs":[],"source":["def run_ia3_bert(dataset):\n","    model_name = \"bert-base-multilingual-cased\"\n","    task_type = TaskType.TOKEN_CLS\n","    training_args = TrainingArguments(\n","        output_dir=f\"ner-ia3-{model_name}\",\n","        learning_rate=1e-1,\n","        per_device_train_batch_size=16,\n","        per_device_eval_batch_size=16,\n","        num_train_epochs=3,\n","        weight_decay=0.01,\n","        evaluation_strategy=\"epoch\",\n","        save_strategy=\"epoch\",\n","        load_best_model_at_end=True,\n","    )\n","    model = AutoModelForTokenClassification.from_pretrained(\n","        model_name, num_labels=9, id2label=id2label, label2id=label2id\n","    )\n","    model = prepare_model_for_kbit_training(model, task_type)\n","\n","    feed_forward_modules = [\n","        \"roberta.encoder.layer.\" + str(i) + \".intermediate.dense\"\n","        for i in range(model.config.num_hidden_layers)\n","    ]\n","\n","    target_modules = (\n","        [\n","            \"bert.encoder.layer.\" + str(i) + \".attention.self.query\"\n","            for i in range(model.config.num_hidden_layers)\n","        ]\n","        + [\n","            \"bert.encoder.layer.\" + str(i) + \".attention.self.key\"\n","            for i in range(model.config.num_hidden_layers)\n","        ]\n","        + [\n","            \"bert.encoder.layer.\" + str(i) + \".attention.self.value\"\n","            for i in range(model.config.num_hidden_layers)\n","        ]\n","        + [\n","            \"bert.encoder.layer.\" + str(i) + \".attention.output.dense\"\n","            for i in range(model.config.num_hidden_layers)\n","        ]\n","        + feed_forward_modules\n","    )\n","\n","    ia3_config = IA3Config(task_type=task_type, feedforward_modules = feed_forward_modules, target_modules=target_modules)\n","\n","    model = get_peft_model(model, ia3_config)\n","\n","    _, metrics, elapsed_training = fine_tune_model(\n","        model_name, dataset, model, training_args\n","    )\n","    print(f\"Training time: {elapsed_training}\")\n","    current_time = time.strftime(\"%Y-%m-%d-%H-%M-%S\")\n","    with open(\"results.csv\", \"a\") as f:\n","        f.write(\n","            f\"{current_time},{model_name},Sentinews, {metrics},{elapsed_training}\\n\"\n","        )"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-03T00:43:20.407551Z","iopub.status.busy":"2024-05-03T00:43:20.406774Z","iopub.status.idle":"2024-05-03T00:46:11.916974Z","shell.execute_reply":"2024-05-03T00:46:11.915874Z","shell.execute_reply.started":"2024-05-03T00:43:20.407522Z"},"trusted":true},"outputs":[],"source":["run_ia3_bert(dataset = {\"train\": train_dataset, \"val\": val_dataset, \"test\": test_dataset})"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30699,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
