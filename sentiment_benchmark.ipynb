{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-29T16:33:50.298939Z","iopub.status.busy":"2024-04-29T16:33:50.298089Z"},"trusted":true},"outputs":[],"source":["import wandb\n","WANDB_API = \"\"\n","wandb.login(key=WANDB_API)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["!pip install -q peft\n","import torch\n","import time\n","import os\n","from transformers import (\n","    AutoModelForSequenceClassification,\n","    AutoTokenizer,\n","    DataCollatorWithPadding,\n","    Trainer,\n","    TrainingArguments,\n",")\n","from sklearn.model_selection import train_test_split\n","from datasets import Dataset\n","from datasets import load_dataset, load_metric\n","from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, TaskType, PrefixTuningConfig, IA3Config\n","from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["dataset = load_dataset(\"cjvt/sentinews\", \"sentence_level\")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["train_dataset = dataset[\"train\"].to_pandas()\n","train_dataset, test_dataset = train_test_split(\n","    train_dataset, test_size=0.2, random_state=42\n",")\n","train_dataset, val_dataset = train_test_split(\n","    train_dataset, test_size=0.1, random_state=42\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["train_dataset = Dataset.from_pandas(train_dataset)\n","val_dataset = Dataset.from_pandas(val_dataset)\n","test_dataset = Dataset.from_pandas(test_dataset)\n","\n","\n","print(train_dataset)\n","print(val_dataset)\n","print(test_dataset)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# For reference\n","models = [\"EMBEDDIA/sloberta\", \"bert-base-multilingual-cased\"]"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def encode_labels(batch_labels):\n","    label_map = {\"negative\": 0, \"neutral\": 1, \"positive\": 2}\n","    return [label_map[label] for label in batch_labels]"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def preprocess_function(examples, model_name):\n","    tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n","    texts = examples[\"content\"]\n","    labels = examples[\"sentiment\"]\n","    tokenized_inputs = tokenizer(texts, truncation=True, padding=True, max_length=512)\n","    tokenized_inputs[\"labels\"] = encode_labels(labels)\n","    return tokenized_inputs"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def compute_metrics(eval_pred):\n","    logits, labels = eval_pred\n","    predictions = np.argmax(logits, axis=-1)\n","    return {\n","        'accuracy': accuracy_score(labels, predictions),\n","        'f1': f1_score(labels, predictions, average='macro'),\n","        'precision': precision_score(labels, predictions, average='macro'),\n","        'recall': recall_score(labels, predictions, average='macro')\n","    }\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def fine_tune_model(model_name, dataset, model, training_args):\n","    tokenized_train_dataset = train_dataset.map(\n","        lambda examples: preprocess_function(examples, model_name),\n","        batched=True,\n","    )\n","    tokenized_val_dataset = val_dataset.map(\n","        lambda examples: preprocess_function(examples, model_name),\n","        batched=True,\n","    )\n","    tokenized_test_dataset = test_dataset.map(\n","        lambda examples: preprocess_function(examples, model_name),\n","        batched=True,\n","    )\n","\n","    data_collator = DataCollatorWithPadding(\n","        tokenizer=AutoTokenizer.from_pretrained(model_name),\n","        padding=\"max_length\",\n","    )\n","\n","    trainer = Trainer(\n","        model=model,\n","        args=training_args,\n","        train_dataset=tokenized_train_dataset,\n","        eval_dataset=tokenized_val_dataset,\n","        data_collator=data_collator,\n","        compute_metrics=compute_metrics,\n","    )\n","\n","    start = time.time()\n","    trainer.train()\n","    elapsed_training = time.time() - start\n","\n","    metrics = trainer.evaluate(tokenized_test_dataset)\n","\n","    print(f\"model: {model_name}, Dataset: Sentinews, Test Metrics: {metrics}\")\n","\n","    model.save_pretrained(f\"models/{model_name}_sentinews\")\n","\n","    return model, metrics, elapsed_training"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def run_lora_sloberta():\n","    model_name = \"EMBEDDIA/sloberta\"\n","    task_type = TaskType.SEQ_CLS\n","    training_args = TrainingArguments(\n","        output_dir=f\"{model_name}-sentinews\",\n","        learning_rate=1e-4,\n","        per_device_train_batch_size=24,\n","        per_device_eval_batch_size=24,\n","        num_train_epochs=3,\n","        weight_decay=0.01,\n","        evaluation_strategy=\"epoch\",\n","        save_strategy=\"epoch\",\n","        load_best_model_at_end=True,\n","    )\n","    model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=3)\n","    model = prepare_model_for_kbit_training(model, task_type)\n","\n","    target_modules = (\n","        [\n","            \"roberta.encoder.layer.\" + str(i) + \".attention.self.query\"\n","            for i in range(model.config.num_hidden_layers)\n","        ]\n","        + [\n","            \"roberta.encoder.layer.\" + str(i) + \".attention.self.key\"\n","            for i in range(model.config.num_hidden_layers)\n","        ]\n","        + [\n","            \"roberta.encoder.layer.\" + str(i) + \".attention.self.value\"\n","            for i in range(model.config.num_hidden_layers)\n","        ]\n","        + [\n","            \"roberta.encoder.layer.\" + str(i) + \".attention.output.dense\"\n","            for i in range(model.config.num_hidden_layers)\n","        ]\n","    )\n","\n","    lora_config = LoraConfig(\n","        r=16,\n","        lora_alpha=32,\n","        lora_dropout=0.05,\n","        task_type=task_type,\n","        bias=\"none\",\n","        target_modules=target_modules,\n","    )\n","\n","    model = get_peft_model(model, lora_config)\n","\n","    _, metrics, elapsed_training = fine_tune_model(\n","        model_name, dataset, model, training_args\n","    )\n","\n","    current_time = time.strftime(\"%Y-%m-%d-%H-%M-%S\")\n","    with open(\"results.csv\", \"a\") as f:\n","        f.write(\n","            f\"{current_time},{model_name},Sentinews, {metrics},{elapsed_training}\\n\"\n","        )"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def run_prefix_tune_sloberta():\n","    model_name = \"EMBEDDIA/sloberta\"\n","    task_type = TaskType.SEQ_CLS\n","    training_args = TrainingArguments(\n","        output_dir=f\"{model_name}-sentinews\",\n","        learning_rate=1e-4,\n","        per_device_train_batch_size=24,\n","        per_device_eval_batch_size=24,\n","        num_train_epochs=3,\n","        weight_decay=0.01,\n","        evaluation_strategy=\"epoch\",\n","        save_strategy=\"epoch\",\n","        load_best_model_at_end=True,\n","    )\n","    model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=3)\n","    model = prepare_model_for_kbit_training(model, task_type)\n","\n","    target_modules = (\n","        [\n","            \"roberta.encoder.layer.\" + str(i) + \".attention.self.query\"\n","            for i in range(model.config.num_hidden_layers)\n","        ]\n","        + [\n","            \"roberta.encoder.layer.\" + str(i) + \".attention.self.key\"\n","            for i in range(model.config.num_hidden_layers)\n","        ]\n","        + [\n","            \"roberta.encoder.layer.\" + str(i) + \".attention.self.value\"\n","            for i in range(model.config.num_hidden_layers)\n","        ]\n","        + [\n","            \"roberta.encoder.layer.\" + str(i) + \".attention.output.dense\"\n","            for i in range(model.config.num_hidden_layers)\n","        ]\n","    )\n","\n","    prefix_config = PrefixTuningConfig(task_type=\"SEQ_CLS\", num_virtual_tokens=20)\n","\n","    model = get_peft_model(model, prefix_config)\n","\n","    _, metrics, elapsed_training = fine_tune_model(\n","        model_name, dataset, model, training_args\n","    )\n","\n","    current_time = time.strftime(\"%Y-%m-%d-%H-%M-%S\")\n","    with open(\"results.csv\", \"a\") as f:\n","        f.write(\n","            f\"{current_time},{model_name},Sentinews, {metrics},{elapsed_training}\\n\"\n","        )"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def run_ia3_sloberta():\n","    model_name = \"EMBEDDIA/sloberta\"\n","    task_type = TaskType.SEQ_CLS\n","    training_args = TrainingArguments(\n","        output_dir=f\"{model_name}-sentinews\",\n","        learning_rate=1e-4,\n","        per_device_train_batch_size=24,\n","        per_device_eval_batch_size=24,\n","        num_train_epochs=3,\n","        weight_decay=0.01,\n","        evaluation_strategy=\"epoch\",\n","        save_strategy=\"epoch\",\n","        load_best_model_at_end=True,\n","    )\n","    model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=3)\n","    model = prepare_model_for_kbit_training(model, task_type)\n","\n","    target_modules = (\n","        [\n","            \"roberta.encoder.layer.\" + str(i) + \".attention.self.query\"\n","            for i in range(model.config.num_hidden_layers)\n","        ]\n","        + [\n","            \"roberta.encoder.layer.\" + str(i) + \".attention.self.key\"\n","            for i in range(model.config.num_hidden_layers)\n","        ]\n","        + [\n","            \"roberta.encoder.layer.\" + str(i) + \".attention.self.value\"\n","            for i in range(model.config.num_hidden_layers)\n","        ]\n","        + [\n","            \"roberta.encoder.layer.\" + str(i) + \".attention.output.dense\"\n","            for i in range(model.config.num_hidden_layers)\n","        ]\n","    )\n","\n","    ia3_config = IA3Config(task_type=task_type, target_modules=target_modules)\n","\n","    model = get_peft_model(model, ia3_config)\n","\n","    _, metrics, elapsed_training = fine_tune_model(\n","        model_name, dataset, model, training_args\n","    )\n","\n","    current_time = time.strftime(\"%Y-%m-%d-%H-%M-%S\")\n","    with open(\"results.csv\", \"a\") as f:\n","        f.write(\n","            f\"{current_time},{model_name},Sentinews, {metrics},{elapsed_training}\\n\"\n","        )"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["run_ia3_sloberta()"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30699,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
