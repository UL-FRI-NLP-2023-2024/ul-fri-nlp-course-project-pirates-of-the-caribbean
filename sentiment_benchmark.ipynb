{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import time\n",
    "import os\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorWithPadding,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import Dataset\n",
    "from datasets import load_dataset, load_metric\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, TaskType\n",
    "import wandb\n",
    "WANDB_API_KEY = os.environ.get(\"WANDB_API_KEY\", \"\")\n",
    "wandb.login(key=WANDB_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"cjvt/sentinews\", \"sentence_level\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['nid', 'content', 'sentiment', 'pid', 'sid'],\n",
       "        num_rows: 168899\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = dataset[\"train\"].to_pandas()\n",
    "train_dataset, test_dataset = train_test_split(\n",
    "    train_dataset, test_size=0.2, random_state=42\n",
    ")\n",
    "train_dataset, val_dataset = train_test_split(\n",
    "    train_dataset, test_size=0.1, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['nid', 'content', 'sentiment', 'pid', 'sid', '__index_level_0__'],\n",
      "    num_rows: 121607\n",
      "})\n",
      "Dataset({\n",
      "    features: ['nid', 'content', 'sentiment', 'pid', 'sid', '__index_level_0__'],\n",
      "    num_rows: 13512\n",
      "})\n",
      "Dataset({\n",
      "    features: ['nid', 'content', 'sentiment', 'pid', 'sid', '__index_level_0__'],\n",
      "    num_rows: 33780\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "train_dataset = Dataset.from_pandas(train_dataset)\n",
    "val_dataset = Dataset.from_pandas(val_dataset)\n",
    "test_dataset = Dataset.from_pandas(test_dataset)\n",
    "\n",
    "\n",
    "print(train_dataset)\n",
    "print(val_dataset)\n",
    "print(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For reference\n",
    "models = [\"EMBEDDIA/sloberta\", \"bert-base-multilingual-cased\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_labels(batch_labels):\n",
    "    label_map = {\"negative\": 0, \"neutral\": 1, \"positive\": 2}\n",
    "    return [label_map[label] for label in batch_labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples, model_name):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "    texts = examples[\"content\"]\n",
    "    labels = examples[\"sentiment\"]\n",
    "    tokenized_inputs = tokenizer(texts, truncation=True, padding=True, max_length=512)\n",
    "    tokenized_inputs[\"labels\"] = encode_labels(labels)\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fine_tune_model(model_name, model, training_args):\n",
    "    tokenized_train_dataset = train_dataset.map(\n",
    "        lambda examples: preprocess_function(examples, model_name),\n",
    "        batched=True,\n",
    "    )\n",
    "    tokenized_val_dataset = val_dataset.map(\n",
    "        lambda examples: preprocess_function(examples, model_name),\n",
    "        batched=True,\n",
    "    )\n",
    "    tokenized_test_dataset = test_dataset.map(\n",
    "        lambda examples: preprocess_function(examples, model_name),\n",
    "        batched=True,\n",
    "    )\n",
    "\n",
    "    data_collator = DataCollatorWithPadding(\n",
    "        tokenizer=AutoTokenizer.from_pretrained(model_name),\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_train_dataset,\n",
    "        eval_dataset=tokenized_val_dataset,\n",
    "        data_collator=data_collator,\n",
    "    )\n",
    "\n",
    "    start = time.time()\n",
    "    trainer.train()\n",
    "    elapsed_training = time.time() - start\n",
    "\n",
    "    metrics = trainer.evaluate(tokenized_test_dataset)\n",
    "\n",
    "    print(f\"model: {model_name}, Dataset: Sentinews, Test Metrics: {metrics}\")\n",
    "\n",
    "    model.save_pretrained(f\"models/{model_name}_sentinews\")\n",
    "\n",
    "    return model, metrics, elapsed_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_lora_sloberta():\n",
    "    model_name = \"EMBEDDIA/sloberta\"\n",
    "    task_type = TaskType.SEQ_CLS\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=f\"{model_name}-sentinews\",\n",
    "        learning_rate=1e-4,\n",
    "        per_device_train_batch_size=64,\n",
    "        per_device_eval_batch_size=64,\n",
    "        num_train_epochs=3,\n",
    "        weight_decay=0.01,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"f1\",\n",
    "        greater_is_better=True,\n",
    "    )\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=3)\n",
    "    model = prepare_model_for_kbit_training(model, task_type)\n",
    "\n",
    "    target_modules = (\n",
    "        [\n",
    "            \"roberta.encoder.layer.\" + str(i) + \".attention.self.query\"\n",
    "            for i in range(model.config.num_hidden_layers)\n",
    "        ]\n",
    "        + [\n",
    "            \"roberta.encoder.layer.\" + str(i) + \".attention.self.key\"\n",
    "            for i in range(model.config.num_hidden_layers)\n",
    "        ]\n",
    "        + [\n",
    "            \"roberta.encoder.layer.\" + str(i) + \".attention.self.value\"\n",
    "            for i in range(model.config.num_hidden_layers)\n",
    "        ]\n",
    "        + [\n",
    "            \"roberta.encoder.layer.\" + str(i) + \".attention.output.dense\"\n",
    "            for i in range(model.config.num_hidden_layers)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    lora_config = LoraConfig(\n",
    "        r=16,\n",
    "        lora_alpha=32,\n",
    "        lora_dropout=0.05,\n",
    "        task_type=task_type,\n",
    "        bias=\"none\",\n",
    "        target_modules=target_modules,\n",
    "    )\n",
    "\n",
    "    model = get_peft_model(model, lora_config)\n",
    "\n",
    "    print_trainable_parameters(model)\n",
    "\n",
    "    _, metrics, elapsed_training = fine_tune_model(\n",
    "        model_name, model, training_args\n",
    "    )\n",
    "\n",
    "    current_time = time.strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
    "    with open(\"results.csv\", \"a\") as f:\n",
    "        f.write(\n",
    "            f\"{current_time},{model_name},Sentinews,{metrics['f1']},{metrics['accuracy']},{elapsed_training}\\n\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of CamembertForSequenceClassification were not initialized from the model checkpoint at EMBEDDIA/sloberta and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 1772547 || all params: 112396806 || trainable%: 1.577043924184109\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "fine_tune_model() takes 3 positional arguments but 4 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[196], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mrun_lora_sloberta\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[195], line 52\u001b[0m, in \u001b[0;36mrun_lora_sloberta\u001b[0;34m()\u001b[0m\n\u001b[1;32m     48\u001b[0m model \u001b[38;5;241m=\u001b[39m get_peft_model(model, lora_config)\n\u001b[1;32m     50\u001b[0m print_trainable_parameters(model)\n\u001b[0;32m---> 52\u001b[0m _, metrics, elapsed_training \u001b[38;5;241m=\u001b[39m \u001b[43mfine_tune_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining_args\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m current_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mstrftime(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY-\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm-\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mH-\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mM-\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mS\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresults.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "\u001b[0;31mTypeError\u001b[0m: fine_tune_model() takes 3 positional arguments but 4 were given"
     ]
    }
   ],
   "source": [
    "run_lora_sloberta()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "benchmark",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
