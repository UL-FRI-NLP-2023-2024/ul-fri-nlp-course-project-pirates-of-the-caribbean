{"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import wandb\nWANDB_API = \"440fbdfe19fc3547947869a7935dd2ad9028815b\"\nwandb.login(key=WANDB_API)\n\nwandb.init(\n    # set the wandb project where this run will be logged\n    project=\"peft\",\n\n    # track hyperparameters and run metadata\n    config={\n    \"architecture\": \"BERT\",\n    \"dataset\": \"ssj500k-\",\n    \"epochs\": 10,\n    \"learning_rate\":2e-5,\n    \"per_device_train_batch_size\":16,\n    \"per_device_eval_batch_size\":16,\n    \"num_train_epochs\":2,\n    \"weight_decay\":0.01,\n    }\n)","metadata":{"execution":{"iopub.status.busy":"2024-05-03T01:08:49.840960Z","iopub.execute_input":"2024-05-03T01:08:49.841578Z","iopub.status.idle":"2024-05-03T01:09:09.871631Z","shell.execute_reply.started":"2024-05-03T01:08:49.841545Z","shell.execute_reply":"2024-05-03T01:09:09.870673Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install seqeval\n!pip install evaluate","metadata":{"execution":{"iopub.status.busy":"2024-05-03T01:09:38.728594Z","iopub.execute_input":"2024-05-03T01:09:38.729287Z","iopub.status.idle":"2024-05-03T01:10:03.230224Z","shell.execute_reply.started":"2024-05-03T01:09:38.729251Z","shell.execute_reply":"2024-05-03T01:10:03.229137Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install -q peft\nimport torch\nimport time\nimport os\nfrom transformers import (\n    AutoModelForTokenClassification,\n    AutoTokenizer,\n    DataCollatorWithPadding,\n    Trainer,\n    TrainingArguments,\n)\nfrom sklearn.model_selection import train_test_split\nfrom datasets import Dataset\nfrom datasets import load_dataset, load_metric\nfrom peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, TaskType, PrefixTuningConfig, IA3Config, PeftModel\nfrom sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score\nimport pandas as pd\nimport numpy as np\nimport evaluate\nseqeval = evaluate.load(\"seqeval\")","metadata":{"execution":{"iopub.status.busy":"2024-05-03T01:10:15.877281Z","iopub.execute_input":"2024-05-03T01:10:15.877658Z","iopub.status.idle":"2024-05-03T01:10:46.958405Z","shell.execute_reply.started":"2024-05-03T01:10:15.877627Z","shell.execute_reply":"2024-05-03T01:10:46.956975Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset = load_dataset(\"cjvt/ssj500k\", \"dependency_parsing_jos\")","metadata":{"execution":{"iopub.status.busy":"2024-05-03T01:10:46.960125Z","iopub.execute_input":"2024-05-03T01:10:46.960680Z","iopub.status.idle":"2024-05-03T01:10:50.352142Z","shell.execute_reply.started":"2024-05-03T01:10:46.960654Z","shell.execute_reply":"2024-05-03T01:10:50.351011Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataset = dataset[\"train\"].to_pandas()\ntrain_dataset, test_dataset = train_test_split(\n    train_dataset, test_size=0.2, random_state=42\n)\ntrain_dataset, val_dataset = train_test_split(\n    train_dataset, test_size=0.1, random_state=42\n)","metadata":{"execution":{"iopub.status.busy":"2024-05-03T01:11:00.154545Z","iopub.execute_input":"2024-05-03T01:11:00.154957Z","iopub.status.idle":"2024-05-03T01:11:00.342579Z","shell.execute_reply.started":"2024-05-03T01:11:00.154913Z","shell.execute_reply":"2024-05-03T01:11:00.341474Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"uni_members = train_dataset['jos_dep_rel'].map(set)\n\nunique_labels = set()\nfor mem in uni_members:\n    unique_labels = unique_labels.union(mem)\n\nunique_labels","metadata":{"execution":{"iopub.status.busy":"2024-05-03T01:11:10.647038Z","iopub.execute_input":"2024-05-03T01:11:10.647847Z","iopub.status.idle":"2024-05-03T01:11:10.708702Z","shell.execute_reply.started":"2024-05-03T01:11:10.647811Z","shell.execute_reply":"2024-05-03T01:11:10.707867Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"id2label = {0: 'AdvM',\n            1: 'AdvO',\n            2: 'Atr',\n            3: 'Conj',\n            4: 'Coord',\n            5: 'MWU',\n            6: 'Obj',\n            7: 'PPart',\n            8: 'Root',\n            9: 'Sb'\n}\n\nlabel2id = {label: id for id,label in id2label.items()}\nnum_labels = len(id2label)\ndisplay(label2id)\ndisplay(id2label)\nprint(f\"Number of labels {num_labels}\")","metadata":{"execution":{"iopub.status.busy":"2024-05-03T01:15:42.868078Z","iopub.execute_input":"2024-05-03T01:15:42.868821Z","iopub.status.idle":"2024-05-03T01:15:42.882037Z","shell.execute_reply.started":"2024-05-03T01:15:42.868790Z","shell.execute_reply":"2024-05-03T01:15:42.880952Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataset = Dataset.from_pandas(train_dataset)\nval_dataset = Dataset.from_pandas(val_dataset)\ntest_dataset = Dataset.from_pandas(test_dataset)\n\n\nprint(train_dataset)\nprint(val_dataset)\nprint(test_dataset)","metadata":{"execution":{"iopub.status.busy":"2024-05-03T01:12:59.609719Z","iopub.execute_input":"2024-05-03T01:12:59.610122Z","iopub.status.idle":"2024-05-03T01:12:59.977629Z","shell.execute_reply.started":"2024-05-03T01:12:59.610094Z","shell.execute_reply":"2024-05-03T01:12:59.976577Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# For reference\nmodels = [\"EMBEDDIA/sloberta\", \"bert-base-multilingual-cased\"]\nmodel_name = models[0]","metadata":{"execution":{"iopub.status.busy":"2024-05-03T01:13:04.628650Z","iopub.execute_input":"2024-05-03T01:13:04.629677Z","iopub.status.idle":"2024-05-03T01:13:04.635676Z","shell.execute_reply.started":"2024-05-03T01:13:04.629622Z","shell.execute_reply":"2024-05-03T01:13:04.634321Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\nfrom transformers import DataCollatorForTokenClassification\ndata_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)","metadata":{"execution":{"iopub.status.busy":"2024-05-03T01:13:06.765087Z","iopub.execute_input":"2024-05-03T01:13:06.765821Z","iopub.status.idle":"2024-05-03T01:13:07.955551Z","shell.execute_reply.started":"2024-05-03T01:13:06.765792Z","shell.execute_reply":"2024-05-03T01:13:07.954431Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def tokenize_and_align_labels(examples, tokenizer):\n    tokenized_inputs = tokenizer(examples[\"words\"], truncation=True, is_split_into_words=True)\n    \n    labels = []\n    for i, label in enumerate(examples[f\"jos_dep_rel\"]):\n        word_ids = tokenized_inputs.word_ids(batch_index=i)  # Map tokens to their respective word.\n        previous_word_idx = None\n        label_ids = []\n        for word_idx in word_ids:  # Set the special tokens to -100.\n            if word_idx is None:\n                label_ids.append(-100)\n            elif word_idx != previous_word_idx:  # Only label the first token of a given word.\n                label_ids.append(label2id[label[word_idx]])\n            else:\n                label_ids.append(-100)\n            previous_word_idx = word_idx\n        labels.append(label_ids)\n\n    tokenized_inputs[\"labels\"] = labels\n    return tokenized_inputs","metadata":{"execution":{"iopub.status.busy":"2024-05-03T01:13:13.210519Z","iopub.execute_input":"2024-05-03T01:13:13.211269Z","iopub.status.idle":"2024-05-03T01:13:13.224655Z","shell.execute_reply.started":"2024-05-03T01:13:13.211236Z","shell.execute_reply":"2024-05-03T01:13:13.223661Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def preprocess_function(examples, tokenizer):\n    tokenized_inputs = tokenize_and_align_labels(examples, tokenizer)\n    return tokenized_inputs","metadata":{"execution":{"iopub.status.busy":"2024-05-03T01:13:16.419498Z","iopub.execute_input":"2024-05-03T01:13:16.420197Z","iopub.status.idle":"2024-05-03T01:13:16.425995Z","shell.execute_reply.started":"2024-05-03T01:13:16.420163Z","shell.execute_reply":"2024-05-03T01:13:16.424801Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\ntokenized_train = train_dataset.map(lambda example: preprocess_function(example, tokenizer), batched=True)","metadata":{"execution":{"iopub.status.busy":"2024-05-03T01:14:07.123488Z","iopub.execute_input":"2024-05-03T01:14:07.123901Z","iopub.status.idle":"2024-05-03T01:14:08.777277Z","shell.execute_reply.started":"2024-05-03T01:14:07.123867Z","shell.execute_reply":"2024-05-03T01:14:08.772489Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def compute_metrics(p):\n    predictions, labels = p\n    predictions = np.argmax(predictions, axis=2)\n\n    true_predictions = [\n        [id2label[p] for (p, l) in zip(prediction, label) if l != -100]\n        for prediction, label in zip(predictions, labels)\n    ]\n    true_labels = [\n        [id2label[l] for (p, l) in zip(prediction, label) if l != -100]\n        for prediction, label in zip(predictions, labels)\n    ]\n\n    results = seqeval.compute(predictions=true_predictions, references=true_labels)\n    result_metrics = {\n        \"precision\": results[\"overall_precision\"],\n        \"recall\": results[\"overall_recall\"],\n        \"f1\": results[\"overall_f1\"],\n        \"accuracy\": results[\"overall_accuracy\"],\n    }\n    wandb.log(result_metrics)\n    return result_metrics","metadata":{"execution":{"iopub.status.busy":"2024-05-03T01:14:14.668023Z","iopub.execute_input":"2024-05-03T01:14:14.668889Z","iopub.status.idle":"2024-05-03T01:14:14.681598Z","shell.execute_reply.started":"2024-05-03T01:14:14.668855Z","shell.execute_reply":"2024-05-03T01:14:14.680519Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def fine_tune_model(model_name, dataset, model, training_args):\n    \n    tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n    print(dataset)\n    \n    tokenized_train_dataset = dataset[\"train\"].map(\n        lambda examples: preprocess_function(examples, tokenizer),\n        batched=True,\n    )\n    tokenized_val_dataset = dataset[\"val\"].map(\n        lambda examples: preprocess_function(examples, tokenizer),\n        batched=True,\n    )\n    tokenized_test_dataset = dataset[\"test\"].map(\n        lambda examples: preprocess_function(examples, tokenizer),\n        batched=True,\n    )\n\n    data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n\n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        train_dataset=tokenized_train_dataset,\n        eval_dataset=tokenized_val_dataset,\n        data_collator=data_collator,\n        compute_metrics=compute_metrics,\n    )\n\n    start = time.time()\n    trainer.train()\n    elapsed_training = time.time() - start\n\n    metrics = trainer.evaluate(tokenized_test_dataset)\n\n    print(f\"model: {model_name}, Dataset: ssj500k, Test Metrics: {metrics}\")\n\n    model.save_pretrained(f\"models/{model_name}_ner_sj500k\")\n\n    return model, metrics, elapsed_training","metadata":{"execution":{"iopub.status.busy":"2024-05-03T01:14:17.388028Z","iopub.execute_input":"2024-05-03T01:14:17.388405Z","iopub.status.idle":"2024-05-03T01:14:17.398974Z","shell.execute_reply.started":"2024-05-03T01:14:17.388376Z","shell.execute_reply":"2024-05-03T01:14:17.397818Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_name = models[0]\nmodel = AutoModelForTokenClassification.from_pretrained(\n    model_name, num_labels=num_labels, id2label=id2label, label2id=label2id\n)\n\ntraining_args = TrainingArguments(\n    output_dir=f\"ner_fully_finetuned_{model_name}\",\n    learning_rate=2e-5,\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=16,\n    num_train_epochs=2,\n    weight_decay=0.01,\n    evaluation_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    load_best_model_at_end=True,\n)\n\ndataset = {\"train\": train_dataset, \"val\": val_dataset, \"test\": test_dataset}\nmodel, metrics, elapsed_training =  fine_tune_model(model_name, dataset, model, training_args=training_args)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def fully_finetune_sloberta():\n    model_name = \"EMBEDDIA/sloberta\"\n    model = AutoModelForTokenClassification.from_pretrained(\n        model_name, num_labels=num_labels, id2label=id2label, label2id=label2id\n    )\n\n    training_args = TrainingArguments(\n        output_dir=f\"ner_fully_finetuned_{model_name}\",\n        learning_rate=2e-5,\n        per_device_train_batch_size=32,\n        per_device_eval_batch_size=32,\n        num_train_epochs=3,\n        weight_decay=0.01,\n        evaluation_strategy=\"epoch\",\n        save_strategy=\"epoch\",\n        load_best_model_at_end=True,\n    )\n\n\n    dataset = {\"train\": train_dataset, \"val\": val_dataset, \"test\": test_dataset}\n    model, metrics, elapsed_training =  fine_tune_model(model_name, dataset, model, training_args=training_args)\n    current_time = time.strftime(\"%Y-%m-%d-%H-%M-%S\")\n    with open(\"results.csv\", \"a\") as f:\n        f.write(\n            f\"{current_time},{model_name},SSJ500-NER, {metrics},{elapsed_training}\\n\"\n        )\n    print(f\"Training time: {elapsed_training}\")\n\nfully_finetune_sloberta()","metadata":{"execution":{"iopub.status.busy":"2024-05-03T01:37:08.326812Z","iopub.execute_input":"2024-05-03T01:37:08.327703Z","iopub.status.idle":"2024-05-03T01:37:25.540315Z","shell.execute_reply.started":"2024-05-03T01:37:08.327668Z","shell.execute_reply":"2024-05-03T01:37:25.539013Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def fully_finetune_bert():\n    model_name = \"bert-base-multilingual-cased\"\n    model = AutoModelForTokenClassification.from_pretrained(\n        model_name, num_labels=num_labels, id2label=id2label, label2id=label2id\n    )\n\n    training_args = TrainingArguments(\n        output_dir=f\"ner_fully_finetuned_{model_name}\",\n        learning_rate=2e-5,\n        per_device_train_batch_size=32,\n        per_device_eval_batch_size=32,\n        num_train_epochs=3,\n        weight_decay=0.01,\n        evaluation_strategy=\"epoch\",\n        save_strategy=\"epoch\",\n        load_best_model_at_end=True,\n    )\n\n    dataset = {\"train\": train_dataset, \"val\": val_dataset, \"test\": test_dataset}\n    model, metrics, elapsed_training =  fine_tune_model(model_name, dataset, model, training_args=training_args)\n    current_time = time.strftime(\"%Y-%m-%d-%H-%M-%S\")\n    with open(\"results.csv\", \"a\") as f:\n        f.write(\n            f\"{current_time},{model_name},SSJ500-NER, {metrics},{elapsed_training}\\n\"\n        )\n    print(f\"Training time: {elapsed_training}\")\n\nfully_finetune_bert()","metadata":{"execution":{"iopub.status.busy":"2024-05-02T23:52:36.814442Z","iopub.execute_input":"2024-05-02T23:52:36.815353Z","iopub.status.idle":"2024-05-02T23:56:59.265842Z","shell.execute_reply.started":"2024-05-02T23:52:36.815320Z","shell.execute_reply":"2024-05-02T23:56:59.264807Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def run_lora_sloberta(dataset):\n    model_name = \"EMBEDDIA/sloberta\"\n    task_type = TaskType.TOKEN_CLS\n    training_args = TrainingArguments(\n        output_dir=f\"ner_lora_finetuned_{model_name}\",\n        learning_rate=1e-3,\n        per_device_train_batch_size=32,\n        per_device_eval_batch_size=32,\n        num_train_epochs=3,\n        weight_decay=0.01,\n        evaluation_strategy=\"epoch\",\n        save_strategy=\"epoch\",\n        load_best_model_at_end=True,\n    )\n    model = AutoModelForTokenClassification.from_pretrained(\n        model_name, num_labels=num_labels, id2label=id2label, label2id=label2id\n    )\n\n    target_modules = (\n        [\n            \"roberta.encoder.layer.\" + str(i) + \".attention.self.query\"\n            for i in range(model.config.num_hidden_layers)\n        ]\n        + [\n            \"roberta.encoder.layer.\" + str(i) + \".attention.self.key\"\n            for i in range(model.config.num_hidden_layers)\n        ]\n        + [\n            \"roberta.encoder.layer.\" + str(i) + \".attention.self.value\"\n            for i in range(model.config.num_hidden_layers)\n        ]\n        + [\n            \"roberta.encoder.layer.\" + str(i) + \".attention.output.dense\"\n            for i in range(model.config.num_hidden_layers)\n        ]\n    )\n    \n    peft_config = LoraConfig(\n        task_type=TaskType.TOKEN_CLS, inference_mode=False, r=16, lora_alpha=16, lora_dropout=0.1, bias=\"all\"\n    )\n\n    lora_config = LoraConfig(\n        r=16,\n        lora_alpha=32,\n        lora_dropout=0.05,\n        task_type=task_type,\n        bias=\"none\",\n        target_modules=target_modules,\n    )\n\n    model = get_peft_model(model, lora_config)\n\n    _, metrics, elapsed_training = fine_tune_model(\n        model_name, dataset, model, training_args\n    )\n    print(f\"Training time: {elapsed_training}\")\n    current_time = time.strftime(\"%Y-%m-%d-%H-%M-%S\")\n    with open(\"results.csv\", \"a\") as f:\n        f.write(\n            f\"{current_time},{model_name},SSJ500-NER, {metrics},{elapsed_training}\\n\"\n        )\n    print(f\"Training time: {elapsed_training}\")","metadata":{"execution":{"iopub.status.busy":"2024-05-03T01:24:00.121087Z","iopub.execute_input":"2024-05-03T01:24:00.121483Z","iopub.status.idle":"2024-05-03T01:24:00.135663Z","shell.execute_reply.started":"2024-05-03T01:24:00.121444Z","shell.execute_reply":"2024-05-03T01:24:00.134835Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"run_lora_sloberta(dataset = {\"train\": train_dataset, \"val\": val_dataset, \"test\": test_dataset})","metadata":{"execution":{"iopub.status.busy":"2024-05-03T01:20:59.109792Z","iopub.execute_input":"2024-05-03T01:20:59.110384Z","iopub.status.idle":"2024-05-03T01:24:00.118689Z","shell.execute_reply.started":"2024-05-03T01:20:59.110349Z","shell.execute_reply":"2024-05-03T01:24:00.117642Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def run_lora_bert(dataset):\n    model_name = \"bert-base-multilingual-cased\"\n    task_type = TaskType.TOKEN_CLS\n    training_args = TrainingArguments(\n        output_dir=f\"ner_lora_finetuned_{model_name}\",\n        learning_rate=1e-3,\n        per_device_train_batch_size=32,\n        per_device_eval_batch_size=32,\n        num_train_epochs=3,\n        weight_decay=0.01,\n        evaluation_strategy=\"epoch\",\n        save_strategy=\"epoch\",\n        load_best_model_at_end=True,\n    )\n    model = AutoModelForTokenClassification.from_pretrained(\n        model_name, num_labels=num_labels, id2label=id2label, label2id=label2id\n    )\n\n    target_modules = (\n        [\n            \"bert.encoder.layer.\" + str(i) + \".attention.self.query\"\n            for i in range(model.config.num_hidden_layers)\n        ]\n        + [\n            \"bert.encoder.layer.\" + str(i) + \".attention.self.key\"\n            for i in range(model.config.num_hidden_layers)\n        ]\n        + [\n            \"bert.encoder.layer.\" + str(i) + \".attention.self.value\"\n            for i in range(model.config.num_hidden_layers)\n        ]\n        + [\n            \"bert.encoder.layer.\" + str(i) + \".attention.output.dense\"\n            for i in range(model.config.num_hidden_layers)\n        ]\n    )\n    \n    peft_config = LoraConfig(\n        task_type=TaskType.TOKEN_CLS, inference_mode=False, r=16, lora_alpha=16, lora_dropout=0.1, bias=\"all\"\n    )\n\n    lora_config = LoraConfig(\n        r=16,\n        lora_alpha=32,\n        lora_dropout=0.05,\n        task_type=task_type,\n        bias=\"none\",\n        target_modules=target_modules,\n    )\n\n    model = get_peft_model(model, lora_config)\n\n    _, metrics, elapsed_training = fine_tune_model(\n        model_name, dataset, model, training_args\n    )\n    print(f\"Training time: {elapsed_training}\")\n    current_time = time.strftime(\"%Y-%m-%d-%H-%M-%S\")\n    with open(\"results.csv\", \"a\") as f:\n        f.write(\n            f\"{current_time},{model_name},SSJ500-NER, {metrics},{elapsed_training}\\n\"\n        )\n    print(f\"Training time: {elapsed_training}\")","metadata":{"execution":{"iopub.status.busy":"2024-05-03T00:13:39.129376Z","iopub.execute_input":"2024-05-03T00:13:39.130135Z","iopub.status.idle":"2024-05-03T00:13:39.143805Z","shell.execute_reply.started":"2024-05-03T00:13:39.130102Z","shell.execute_reply":"2024-05-03T00:13:39.142737Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"run_lora_bert(dataset = {\"train\": train_dataset, \"val\": val_dataset, \"test\": test_dataset})","metadata":{"execution":{"iopub.status.busy":"2024-05-03T00:13:48.263948Z","iopub.execute_input":"2024-05-03T00:13:48.264369Z","iopub.status.idle":"2024-05-03T00:17:04.343895Z","shell.execute_reply.started":"2024-05-03T00:13:48.264340Z","shell.execute_reply":"2024-05-03T00:17:04.342959Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def run_prefix_tune_sloberta(dataset):\n    model_name = \"EMBEDDIA/sloberta\"\n\n    task_type = TaskType.TOKEN_CLS\n    training_args = TrainingArguments(\n        output_dir=f\"ner_prefix_tunning_finetuned_{model_name}\",\n        learning_rate=1e-2,\n        per_device_train_batch_size=32,\n        per_device_eval_batch_size=32,\n        num_train_epochs=3,\n        weight_decay=0.01,\n        evaluation_strategy=\"epoch\",\n        save_strategy=\"epoch\",\n        load_best_model_at_end=True,\n    )\n    model = AutoModelForTokenClassification.from_pretrained(\n        model_name, num_labels=num_labels, id2label=id2label, label2id=label2id\n    )\n\n    target_modules = (\n        [\n            \"roberta.encoder.layer.\" + str(i) + \".attention.self.query\"\n            for i in range(model.config.num_hidden_layers)\n        ]\n        + [\n            \"roberta.encoder.layer.\" + str(i) + \".attention.self.key\"\n            for i in range(model.config.num_hidden_layers)\n        ]\n        + [\n            \"roberta.encoder.layer.\" + str(i) + \".attention.self.value\"\n            for i in range(model.config.num_hidden_layers)\n        ]\n        + [\n            \"roberta.encoder.layer.\" + str(i) + \".attention.output.dense\"\n            for i in range(model.config.num_hidden_layers)\n        ]\n    )\n\n    prefix_config = PrefixTuningConfig(task_type=\"TOKEN_CLS\", num_virtual_tokens=20)\n\n    model = get_peft_model(model, prefix_config)\n\n    _, metrics, elapsed_training = fine_tune_model(\n        model_name, dataset, model, training_args\n    )\n    print(f\"Training time: {elapsed_training}\")\n    current_time = time.strftime(\"%Y-%m-%d-%H-%M-%S\")\n    with open(\"results.csv\", \"a\") as f:\n        f.write(\n            f\"{current_time},{model_name},SSJ500-NER, {metrics},{elapsed_training}\\n\"\n        )","metadata":{"execution":{"iopub.status.busy":"2024-05-02T22:23:25.816578Z","iopub.execute_input":"2024-05-02T22:23:25.817289Z","iopub.status.idle":"2024-05-02T22:23:25.829406Z","shell.execute_reply.started":"2024-05-02T22:23:25.817255Z","shell.execute_reply":"2024-05-02T22:23:25.828227Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"run_prefix_tune_sloberta(dataset = {\"train\": train_dataset, \"val\": val_dataset, \"test\": test_dataset})","metadata":{"execution":{"iopub.status.busy":"2024-05-02T22:23:28.985436Z","iopub.execute_input":"2024-05-02T22:23:28.986322Z","iopub.status.idle":"2024-05-02T22:25:35.259769Z","shell.execute_reply.started":"2024-05-02T22:23:28.986288Z","shell.execute_reply":"2024-05-02T22:25:35.258668Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def run_prefix_tune_bert(dataset):\n    model_name = \"bert-base-multilingual-cased\"\n\n    task_type = TaskType.TOKEN_CLS\n    training_args = TrainingArguments(\n        output_dir=f\"ner_prefix_tunning_finetuned_{model_name}\",\n        learning_rate=1e-2,\n        per_device_train_batch_size=32,\n        per_device_eval_batch_size=32,\n        num_train_epochs=3,\n        weight_decay=0.01,\n        evaluation_strategy=\"epoch\",\n        save_strategy=\"epoch\",\n        load_best_model_at_end=True,\n    )\n    model = AutoModelForTokenClassification.from_pretrained(\n        model_name, num_labels=num_labels, id2label=id2label, label2id=label2id\n    )\n\n    target_modules = (\n        [\n            \"bert.encoder.layer.\" + str(i) + \".attention.self.query\"\n            for i in range(model.config.num_hidden_layers)\n        ]\n        + [\n            \"bert.encoder.layer.\" + str(i) + \".attention.self.key\"\n            for i in range(model.config.num_hidden_layers)\n        ]\n        + [\n            \"bert.encoder.layer.\" + str(i) + \".attention.self.value\"\n            for i in range(model.config.num_hidden_layers)\n        ]\n        + [\n            \"bert.encoder.layer.\" + str(i) + \".attention.output.dense\"\n            for i in range(model.config.num_hidden_layers)\n        ]\n    )\n\n    prefix_config = PrefixTuningConfig(task_type=\"TOKEN_CLS\", num_virtual_tokens=20)\n\n    model = get_peft_model(model, prefix_config)\n\n    _, metrics, elapsed_training = fine_tune_model(\n        model_name, dataset, model, training_args\n    )\n    print(f\"Training time: {elapsed_training}\")\n    current_time = time.strftime(\"%Y-%m-%d-%H-%M-%S\")\n    with open(\"results.csv\", \"a\") as f:\n        f.write(\n            f\"{current_time},{model_name},SSJ500-NER, {metrics},{elapsed_training}\\n\"\n        )","metadata":{"execution":{"iopub.status.busy":"2024-05-03T00:19:05.521620Z","iopub.execute_input":"2024-05-03T00:19:05.522067Z","iopub.status.idle":"2024-05-03T00:19:05.534420Z","shell.execute_reply.started":"2024-05-03T00:19:05.522036Z","shell.execute_reply":"2024-05-03T00:19:05.533009Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"run_prefix_tune_bert(dataset = {\"train\": train_dataset, \"val\": val_dataset, \"test\": test_dataset})","metadata":{"execution":{"iopub.status.busy":"2024-05-03T00:19:10.912690Z","iopub.execute_input":"2024-05-03T00:19:10.913557Z","iopub.status.idle":"2024-05-03T00:21:55.720373Z","shell.execute_reply.started":"2024-05-03T00:19:10.913525Z","shell.execute_reply":"2024-05-03T00:21:55.718994Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def run_ia3_sloberta(dataset):\n    model_name = \"EMBEDDIA/sloberta\"\n    task_type = TaskType.TOKEN_CLS\n    training_args = TrainingArguments(\n        output_dir=f\"ner-ia3-{model_name}\",\n        learning_rate=1e-2,\n        per_device_train_batch_size=32,\n        per_device_eval_batch_size=32,\n        num_train_epochs=3,\n        weight_decay=0.01,\n        evaluation_strategy=\"epoch\",\n        save_strategy=\"epoch\",\n        load_best_model_at_end=True,\n    )\n    model = AutoModelForTokenClassification.from_pretrained(\n        model_name, num_labels=num_labels, id2label=id2label, label2id=label2id\n    )\n    model = prepare_model_for_kbit_training(model, task_type)\n\n    feed_forward_modules = [\n        \"roberta.encoder.layer.\" + str(i) + \".intermediate.dense\"\n        for i in range(model.config.num_hidden_layers)\n    ]\n\n    target_modules = (\n        [\n            \"roberta.encoder.layer.\" + str(i) + \".attention.self.query\"\n            for i in range(model.config.num_hidden_layers)\n        ]\n        + [\n            \"roberta.encoder.layer.\" + str(i) + \".attention.self.key\"\n            for i in range(model.config.num_hidden_layers)\n        ]\n        + [\n            \"roberta.encoder.layer.\" + str(i) + \".attention.self.value\"\n            for i in range(model.config.num_hidden_layers)\n        ]\n        + [\n            \"roberta.encoder.layer.\" + str(i) + \".attention.output.dense\"\n            for i in range(model.config.num_hidden_layers)\n        ]\n        + feed_forward_modules\n    )\n\n    ia3_config = IA3Config(task_type=task_type, feedforward_modules = feed_forward_modules, target_modules=target_modules)\n\n    model = get_peft_model(model, ia3_config)\n\n    _, metrics, elapsed_training = fine_tune_model(\n        model_name, dataset, model, training_args\n    )\n    print(f\"Training time: {elapsed_training}\")\n    current_time = time.strftime(\"%Y-%m-%d-%H-%M-%S\")\n    with open(\"results.csv\", \"a\") as f:\n        f.write(\n            f\"{current_time},{model_name},Sentinews, {metrics},{elapsed_training}\\n\"\n        )","metadata":{"execution":{"iopub.status.busy":"2024-05-03T00:23:34.315305Z","iopub.execute_input":"2024-05-03T00:23:34.316203Z","iopub.status.idle":"2024-05-03T00:23:34.334468Z","shell.execute_reply.started":"2024-05-03T00:23:34.316166Z","shell.execute_reply":"2024-05-03T00:23:34.333395Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"run_ia3_sloberta(dataset = {\"train\": train_dataset, \"val\": val_dataset, \"test\": test_dataset})","metadata":{"execution":{"iopub.status.busy":"2024-05-03T00:23:37.178845Z","iopub.execute_input":"2024-05-03T00:23:37.179203Z","iopub.status.idle":"2024-05-03T00:23:44.831721Z","shell.execute_reply.started":"2024-05-03T00:23:37.179176Z","shell.execute_reply":"2024-05-03T00:23:44.830045Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def run_ia3_bert(dataset):\n    model_name = \"bert-base-multilingual-cased\"\n    task_type = TaskType.TOKEN_CLS\n    training_args = TrainingArguments(\n        output_dir=f\"ner-ia3-{model_name}\",\n        learning_rate=1e-1,\n        per_device_train_batch_size=32,\n        per_device_eval_batch_size=32,\n        num_train_epochs=3,\n        weight_decay=0.01,\n        evaluation_strategy=\"epoch\",\n        save_strategy=\"epoch\",\n        load_best_model_at_end=True,\n    )\n    model = AutoModelForTokenClassification.from_pretrained(\n        model_name, num_labels=num_labels, id2label=id2label, label2id=label2id\n    )\n    model = prepare_model_for_kbit_training(model, task_type)\n\n    feed_forward_modules = [\n        \"roberta.encoder.layer.\" + str(i) + \".intermediate.dense\"\n        for i in range(model.config.num_hidden_layers)\n    ]\n\n    target_modules = (\n        [\n            \"bert.encoder.layer.\" + str(i) + \".attention.self.query\"\n            for i in range(model.config.num_hidden_layers)\n        ]\n        + [\n            \"bert.encoder.layer.\" + str(i) + \".attention.self.key\"\n            for i in range(model.config.num_hidden_layers)\n        ]\n        + [\n            \"bert.encoder.layer.\" + str(i) + \".attention.self.value\"\n            for i in range(model.config.num_hidden_layers)\n        ]\n        + [\n            \"bert.encoder.layer.\" + str(i) + \".attention.output.dense\"\n            for i in range(model.config.num_hidden_layers)\n        ]\n        + feed_forward_modules\n    )\n\n    ia3_config = IA3Config(task_type=task_type, feedforward_modules = feed_forward_modules, target_modules=target_modules)\n\n    model = get_peft_model(model, ia3_config)\n\n    _, metrics, elapsed_training = fine_tune_model(\n        model_name, dataset, model, training_args\n    )\n    print(f\"Training time: {elapsed_training}\")\n    current_time = time.strftime(\"%Y-%m-%d-%H-%M-%S\")\n    with open(\"results.csv\", \"a\") as f:\n        f.write(\n            f\"{current_time},{model_name},Sentinews, {metrics},{elapsed_training}\\n\"\n        )","metadata":{"execution":{"iopub.status.busy":"2024-05-03T00:43:17.364141Z","iopub.execute_input":"2024-05-03T00:43:17.364862Z","iopub.status.idle":"2024-05-03T00:43:17.376705Z","shell.execute_reply.started":"2024-05-03T00:43:17.364832Z","shell.execute_reply":"2024-05-03T00:43:17.375657Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"run_ia3_bert(dataset = {\"train\": train_dataset, \"val\": val_dataset, \"test\": test_dataset})","metadata":{"execution":{"iopub.status.busy":"2024-05-03T00:43:20.406774Z","iopub.execute_input":"2024-05-03T00:43:20.407551Z","iopub.status.idle":"2024-05-03T00:46:11.916974Z","shell.execute_reply.started":"2024-05-03T00:43:20.407522Z","shell.execute_reply":"2024-05-03T00:46:11.915874Z"},"trusted":true},"execution_count":null,"outputs":[]}]}