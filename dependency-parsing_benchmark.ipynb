{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-03T01:08:49.841578Z","iopub.status.busy":"2024-05-03T01:08:49.840960Z","iopub.status.idle":"2024-05-03T01:09:09.871631Z","shell.execute_reply":"2024-05-03T01:09:09.870673Z","shell.execute_reply.started":"2024-05-03T01:08:49.841545Z"},"trusted":true},"outputs":[],"source":["import wandb\n","WANDB_API = \"\"\n","wandb.login(key=WANDB_API)\n","\n","wandb.init(\n","    # set the wandb project where this run will be logged\n","    project=\"peft\",\n","\n","    # track hyperparameters and run metadata\n","    config={\n","    \"architecture\": \"BERT\",\n","    \"dataset\": \"ssj500k-\",\n","    \"epochs\": 10,\n","    \"learning_rate\":2e-5,\n","    \"per_device_train_batch_size\":16,\n","    \"per_device_eval_batch_size\":16,\n","    \"num_train_epochs\":2,\n","    \"weight_decay\":0.01,\n","    }\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-03T01:09:38.729287Z","iopub.status.busy":"2024-05-03T01:09:38.728594Z","iopub.status.idle":"2024-05-03T01:10:03.230224Z","shell.execute_reply":"2024-05-03T01:10:03.229137Z","shell.execute_reply.started":"2024-05-03T01:09:38.729251Z"},"trusted":true},"outputs":[],"source":["!pip install seqeval\n","!pip install evaluate"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-03T01:10:15.877658Z","iopub.status.busy":"2024-05-03T01:10:15.877281Z","iopub.status.idle":"2024-05-03T01:10:46.958405Z","shell.execute_reply":"2024-05-03T01:10:46.956975Z","shell.execute_reply.started":"2024-05-03T01:10:15.877627Z"},"trusted":true},"outputs":[],"source":["!pip install -q peft\n","import torch\n","import time\n","import os\n","from transformers import (\n","    AutoModelForTokenClassification,\n","    AutoTokenizer,\n","    DataCollatorWithPadding,\n","    Trainer,\n","    TrainingArguments,\n",")\n","from sklearn.model_selection import train_test_split\n","from datasets import Dataset\n","from datasets import load_dataset, load_metric\n","from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, TaskType, PrefixTuningConfig, IA3Config, PeftModel\n","from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score\n","import pandas as pd\n","import numpy as np\n","import evaluate\n","seqeval = evaluate.load(\"seqeval\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-03T01:10:46.960680Z","iopub.status.busy":"2024-05-03T01:10:46.960125Z","iopub.status.idle":"2024-05-03T01:10:50.352142Z","shell.execute_reply":"2024-05-03T01:10:50.351011Z","shell.execute_reply.started":"2024-05-03T01:10:46.960654Z"},"trusted":true},"outputs":[],"source":["dataset = load_dataset(\"cjvt/ssj500k\", \"dependency_parsing_jos\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-03T01:11:00.154957Z","iopub.status.busy":"2024-05-03T01:11:00.154545Z","iopub.status.idle":"2024-05-03T01:11:00.342579Z","shell.execute_reply":"2024-05-03T01:11:00.341474Z","shell.execute_reply.started":"2024-05-03T01:11:00.154913Z"},"trusted":true},"outputs":[],"source":["train_dataset = dataset[\"train\"].to_pandas()\n","train_dataset, test_dataset = train_test_split(\n","    train_dataset, test_size=0.2, random_state=42\n",")\n","train_dataset, val_dataset = train_test_split(\n","    train_dataset, test_size=0.1, random_state=42\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-03T01:11:10.647847Z","iopub.status.busy":"2024-05-03T01:11:10.647038Z","iopub.status.idle":"2024-05-03T01:11:10.708702Z","shell.execute_reply":"2024-05-03T01:11:10.707867Z","shell.execute_reply.started":"2024-05-03T01:11:10.647811Z"},"trusted":true},"outputs":[],"source":["uni_members = train_dataset['jos_dep_rel'].map(set)\n","\n","unique_labels = set()\n","for mem in uni_members:\n","    unique_labels = unique_labels.union(mem)\n","\n","unique_labels"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-03T01:15:42.868821Z","iopub.status.busy":"2024-05-03T01:15:42.868078Z","iopub.status.idle":"2024-05-03T01:15:42.882037Z","shell.execute_reply":"2024-05-03T01:15:42.880952Z","shell.execute_reply.started":"2024-05-03T01:15:42.868790Z"},"trusted":true},"outputs":[],"source":["id2label = {0: 'AdvM',\n","            1: 'AdvO',\n","            2: 'Atr',\n","            3: 'Conj',\n","            4: 'Coord',\n","            5: 'MWU',\n","            6: 'Obj',\n","            7: 'PPart',\n","            8: 'Root',\n","            9: 'Sb'\n","}\n","\n","label2id = {label: id for id,label in id2label.items()}\n","num_labels = len(id2label)\n","display(label2id)\n","display(id2label)\n","print(f\"Number of labels {num_labels}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-03T01:12:59.610122Z","iopub.status.busy":"2024-05-03T01:12:59.609719Z","iopub.status.idle":"2024-05-03T01:12:59.977629Z","shell.execute_reply":"2024-05-03T01:12:59.976577Z","shell.execute_reply.started":"2024-05-03T01:12:59.610094Z"},"trusted":true},"outputs":[],"source":["train_dataset = Dataset.from_pandas(train_dataset)\n","val_dataset = Dataset.from_pandas(val_dataset)\n","test_dataset = Dataset.from_pandas(test_dataset)\n","\n","\n","print(train_dataset)\n","print(val_dataset)\n","print(test_dataset)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-03T01:13:04.629677Z","iopub.status.busy":"2024-05-03T01:13:04.628650Z","iopub.status.idle":"2024-05-03T01:13:04.635676Z","shell.execute_reply":"2024-05-03T01:13:04.634321Z","shell.execute_reply.started":"2024-05-03T01:13:04.629622Z"},"trusted":true},"outputs":[],"source":["# For reference\n","models = [\"EMBEDDIA/sloberta\", \"bert-base-multilingual-cased\"]\n","model_name = models[0]"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-03T01:13:06.765821Z","iopub.status.busy":"2024-05-03T01:13:06.765087Z","iopub.status.idle":"2024-05-03T01:13:07.955551Z","shell.execute_reply":"2024-05-03T01:13:07.954431Z","shell.execute_reply.started":"2024-05-03T01:13:06.765792Z"},"trusted":true},"outputs":[],"source":["tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n","from transformers import DataCollatorForTokenClassification\n","data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-03T01:13:13.211269Z","iopub.status.busy":"2024-05-03T01:13:13.210519Z","iopub.status.idle":"2024-05-03T01:13:13.224655Z","shell.execute_reply":"2024-05-03T01:13:13.223661Z","shell.execute_reply.started":"2024-05-03T01:13:13.211236Z"},"trusted":true},"outputs":[],"source":["def tokenize_and_align_labels(examples, tokenizer):\n","    tokenized_inputs = tokenizer(examples[\"words\"], truncation=True, is_split_into_words=True)\n","    \n","    labels = []\n","    for i, label in enumerate(examples[f\"jos_dep_rel\"]):\n","        word_ids = tokenized_inputs.word_ids(batch_index=i)  # Map tokens to their respective word.\n","        previous_word_idx = None\n","        label_ids = []\n","        for word_idx in word_ids:  # Set the special tokens to -100.\n","            if word_idx is None:\n","                label_ids.append(-100)\n","            elif word_idx != previous_word_idx:  # Only label the first token of a given word.\n","                label_ids.append(label2id[label[word_idx]])\n","            else:\n","                label_ids.append(-100)\n","            previous_word_idx = word_idx\n","        labels.append(label_ids)\n","\n","    tokenized_inputs[\"labels\"] = labels\n","    return tokenized_inputs"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-03T01:13:16.420197Z","iopub.status.busy":"2024-05-03T01:13:16.419498Z","iopub.status.idle":"2024-05-03T01:13:16.425995Z","shell.execute_reply":"2024-05-03T01:13:16.424801Z","shell.execute_reply.started":"2024-05-03T01:13:16.420163Z"},"trusted":true},"outputs":[],"source":["def preprocess_function(examples, tokenizer):\n","    tokenized_inputs = tokenize_and_align_labels(examples, tokenizer)\n","    return tokenized_inputs"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-03T01:14:07.123901Z","iopub.status.busy":"2024-05-03T01:14:07.123488Z","iopub.status.idle":"2024-05-03T01:14:08.777277Z","shell.execute_reply":"2024-05-03T01:14:08.772489Z","shell.execute_reply.started":"2024-05-03T01:14:07.123867Z"},"trusted":true},"outputs":[],"source":["tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n","tokenized_train = train_dataset.map(lambda example: preprocess_function(example, tokenizer), batched=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-03T01:14:14.668889Z","iopub.status.busy":"2024-05-03T01:14:14.668023Z","iopub.status.idle":"2024-05-03T01:14:14.681598Z","shell.execute_reply":"2024-05-03T01:14:14.680519Z","shell.execute_reply.started":"2024-05-03T01:14:14.668855Z"},"trusted":true},"outputs":[],"source":["def compute_metrics(p):\n","    predictions, labels = p\n","    predictions = np.argmax(predictions, axis=2)\n","\n","    true_predictions = [\n","        [id2label[p] for (p, l) in zip(prediction, label) if l != -100]\n","        for prediction, label in zip(predictions, labels)\n","    ]\n","    true_labels = [\n","        [id2label[l] for (p, l) in zip(prediction, label) if l != -100]\n","        for prediction, label in zip(predictions, labels)\n","    ]\n","\n","    results = seqeval.compute(predictions=true_predictions, references=true_labels)\n","    result_metrics = {\n","        \"precision\": results[\"overall_precision\"],\n","        \"recall\": results[\"overall_recall\"],\n","        \"f1\": results[\"overall_f1\"],\n","        \"accuracy\": results[\"overall_accuracy\"],\n","    }\n","    wandb.log(result_metrics)\n","    return result_metrics"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-03T01:14:17.388405Z","iopub.status.busy":"2024-05-03T01:14:17.388028Z","iopub.status.idle":"2024-05-03T01:14:17.398974Z","shell.execute_reply":"2024-05-03T01:14:17.397818Z","shell.execute_reply.started":"2024-05-03T01:14:17.388376Z"},"trusted":true},"outputs":[],"source":["def fine_tune_model(model_name, dataset, model, training_args):\n","    \n","    tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n","    print(dataset)\n","    \n","    tokenized_train_dataset = dataset[\"train\"].map(\n","        lambda examples: preprocess_function(examples, tokenizer),\n","        batched=True,\n","    )\n","    tokenized_val_dataset = dataset[\"val\"].map(\n","        lambda examples: preprocess_function(examples, tokenizer),\n","        batched=True,\n","    )\n","    tokenized_test_dataset = dataset[\"test\"].map(\n","        lambda examples: preprocess_function(examples, tokenizer),\n","        batched=True,\n","    )\n","\n","    data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n","\n","    trainer = Trainer(\n","        model=model,\n","        args=training_args,\n","        train_dataset=tokenized_train_dataset,\n","        eval_dataset=tokenized_val_dataset,\n","        data_collator=data_collator,\n","        compute_metrics=compute_metrics,\n","    )\n","\n","    start = time.time()\n","    trainer.train()\n","    elapsed_training = time.time() - start\n","\n","    metrics = trainer.evaluate(tokenized_test_dataset)\n","\n","    print(f\"model: {model_name}, Dataset: ssj500k, Test Metrics: {metrics}\")\n","\n","    model.save_pretrained(f\"models/{model_name}_ner_sj500k\")\n","\n","    return model, metrics, elapsed_training"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["model_name = models[0]\n","model = AutoModelForTokenClassification.from_pretrained(\n","    model_name, num_labels=num_labels, id2label=id2label, label2id=label2id\n",")\n","\n","training_args = TrainingArguments(\n","    output_dir=f\"ner_fully_finetuned_{model_name}\",\n","    learning_rate=2e-5,\n","    per_device_train_batch_size=16,\n","    per_device_eval_batch_size=16,\n","    num_train_epochs=2,\n","    weight_decay=0.01,\n","    evaluation_strategy=\"epoch\",\n","    save_strategy=\"epoch\",\n","    load_best_model_at_end=True,\n",")\n","\n","wandb.config[\"base_model\"] = model_name\n","\n","dataset = {\"train\": train_dataset, \"val\": val_dataset, \"test\": test_dataset}\n","model, metrics, elapsed_training =  fine_tune_model(model_name, dataset, model, training_args=training_args)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-03T01:37:08.327703Z","iopub.status.busy":"2024-05-03T01:37:08.326812Z","iopub.status.idle":"2024-05-03T01:37:25.540315Z","shell.execute_reply":"2024-05-03T01:37:25.539013Z","shell.execute_reply.started":"2024-05-03T01:37:08.327668Z"},"trusted":true},"outputs":[],"source":["def fully_finetune_sloberta():\n","    model_name = \"EMBEDDIA/sloberta\"\n","    model = AutoModelForTokenClassification.from_pretrained(\n","        model_name, num_labels=num_labels, id2label=id2label, label2id=label2id\n","    )\n","\n","    training_args = TrainingArguments(\n","        output_dir=f\"ner_fully_finetuned_{model_name}\",\n","        learning_rate=2e-5,\n","        per_device_train_batch_size=32,\n","        per_device_eval_batch_size=32,\n","        num_train_epochs=3,\n","        weight_decay=0.01,\n","        evaluation_strategy=\"epoch\",\n","        save_strategy=\"epoch\",\n","        load_best_model_at_end=True,\n","    )\n","\n","\n","    dataset = {\"train\": train_dataset, \"val\": val_dataset, \"test\": test_dataset}\n","    model, metrics, elapsed_training =  fine_tune_model(model_name, dataset, model, training_args=training_args)\n","    current_time = time.strftime(\"%Y-%m-%d-%H-%M-%S\")\n","    with open(\"results.csv\", \"a\") as f:\n","        f.write(\n","            f\"{current_time},{model_name},SSJ500-NER, {metrics},{elapsed_training}\\n\"\n","        )\n","    print(f\"Training time: {elapsed_training}\")\n","\n","fully_finetune_sloberta()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-02T23:52:36.815353Z","iopub.status.busy":"2024-05-02T23:52:36.814442Z","iopub.status.idle":"2024-05-02T23:56:59.265842Z","shell.execute_reply":"2024-05-02T23:56:59.264807Z","shell.execute_reply.started":"2024-05-02T23:52:36.815320Z"},"trusted":true},"outputs":[],"source":["def fully_finetune_bert():\n","    model_name = \"bert-base-multilingual-cased\"\n","    model = AutoModelForTokenClassification.from_pretrained(\n","        model_name, num_labels=num_labels, id2label=id2label, label2id=label2id\n","    )\n","\n","    training_args = TrainingArguments(\n","        output_dir=f\"ner_fully_finetuned_{model_name}\",\n","        learning_rate=2e-5,\n","        per_device_train_batch_size=32,\n","        per_device_eval_batch_size=32,\n","        num_train_epochs=3,\n","        weight_decay=0.01,\n","        evaluation_strategy=\"epoch\",\n","        save_strategy=\"epoch\",\n","        load_best_model_at_end=True,\n","    )\n","\n","    wandb.config[\"base_model\"] = model_name\n","\n","    dataset = {\"train\": train_dataset, \"val\": val_dataset, \"test\": test_dataset}\n","    model, metrics, elapsed_training =  fine_tune_model(model_name, dataset, model, training_args=training_args)\n","    current_time = time.strftime(\"%Y-%m-%d-%H-%M-%S\")\n","    with open(\"results.csv\", \"a\") as f:\n","        f.write(\n","            f\"{current_time},{model_name},SSJ500-NER, {metrics},{elapsed_training}\\n\"\n","        )\n","    print(f\"Training time: {elapsed_training}\")\n","\n","fully_finetune_bert()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-03T01:24:00.121483Z","iopub.status.busy":"2024-05-03T01:24:00.121087Z","iopub.status.idle":"2024-05-03T01:24:00.135663Z","shell.execute_reply":"2024-05-03T01:24:00.134835Z","shell.execute_reply.started":"2024-05-03T01:24:00.121444Z"},"trusted":true},"outputs":[],"source":["def run_lora_sloberta(dataset):\n","    model_name = \"EMBEDDIA/sloberta\"\n","    task_type = TaskType.TOKEN_CLS\n","    training_args = TrainingArguments(\n","        output_dir=f\"ner_lora_finetuned_{model_name}\",\n","        learning_rate=1e-3,\n","        per_device_train_batch_size=32,\n","        per_device_eval_batch_size=32,\n","        num_train_epochs=3,\n","        weight_decay=0.01,\n","        evaluation_strategy=\"epoch\",\n","        save_strategy=\"epoch\",\n","        load_best_model_at_end=True,\n","    )\n","    model = AutoModelForTokenClassification.from_pretrained(\n","        model_name, num_labels=num_labels, id2label=id2label, label2id=label2id\n","    )\n","\n","    target_modules = (\n","        [\n","            \"roberta.encoder.layer.\" + str(i) + \".attention.self.query\"\n","            for i in range(model.config.num_hidden_layers)\n","        ]\n","        + [\n","            \"roberta.encoder.layer.\" + str(i) + \".attention.self.key\"\n","            for i in range(model.config.num_hidden_layers)\n","        ]\n","        + [\n","            \"roberta.encoder.layer.\" + str(i) + \".attention.self.value\"\n","            for i in range(model.config.num_hidden_layers)\n","        ]\n","        + [\n","            \"roberta.encoder.layer.\" + str(i) + \".attention.output.dense\"\n","            for i in range(model.config.num_hidden_layers)\n","        ]\n","    )\n","    \n","    peft_config = LoraConfig(\n","        task_type=TaskType.TOKEN_CLS, inference_mode=False, r=16, lora_alpha=16, lora_dropout=0.1, bias=\"all\"\n","    )\n","\n","    lora_config = LoraConfig(\n","        r=16,\n","        lora_alpha=32,\n","        lora_dropout=0.05,\n","        task_type=task_type,\n","        bias=\"none\",\n","        target_modules=target_modules,\n","    )\n","\n","    model = get_peft_model(model, lora_config)\n","\n","    _, metrics, elapsed_training = fine_tune_model(\n","        model_name, dataset, model, training_args\n","    )\n","    print(f\"Training time: {elapsed_training}\")\n","    current_time = time.strftime(\"%Y-%m-%d-%H-%M-%S\")\n","    with open(\"results.csv\", \"a\") as f:\n","        f.write(\n","            f\"{current_time},{model_name},SSJ500-NER, {metrics},{elapsed_training}\\n\"\n","        )\n","    print(f\"Training time: {elapsed_training}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-03T01:20:59.110384Z","iopub.status.busy":"2024-05-03T01:20:59.109792Z","iopub.status.idle":"2024-05-03T01:24:00.118689Z","shell.execute_reply":"2024-05-03T01:24:00.117642Z","shell.execute_reply.started":"2024-05-03T01:20:59.110349Z"},"trusted":true},"outputs":[],"source":["run_lora_sloberta(dataset = {\"train\": train_dataset, \"val\": val_dataset, \"test\": test_dataset})"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-03T00:13:39.130135Z","iopub.status.busy":"2024-05-03T00:13:39.129376Z","iopub.status.idle":"2024-05-03T00:13:39.143805Z","shell.execute_reply":"2024-05-03T00:13:39.142737Z","shell.execute_reply.started":"2024-05-03T00:13:39.130102Z"},"trusted":true},"outputs":[],"source":["def run_lora_bert(dataset):\n","    model_name = \"bert-base-multilingual-cased\"\n","    task_type = TaskType.TOKEN_CLS\n","    training_args = TrainingArguments(\n","        output_dir=f\"ner_lora_finetuned_{model_name}\",\n","        learning_rate=1e-3,\n","        per_device_train_batch_size=32,\n","        per_device_eval_batch_size=32,\n","        num_train_epochs=3,\n","        weight_decay=0.01,\n","        evaluation_strategy=\"epoch\",\n","        save_strategy=\"epoch\",\n","        load_best_model_at_end=True,\n","    )\n","    model = AutoModelForTokenClassification.from_pretrained(\n","        model_name, num_labels=num_labels, id2label=id2label, label2id=label2id\n","    )\n","\n","    target_modules = (\n","        [\n","            \"bert.encoder.layer.\" + str(i) + \".attention.self.query\"\n","            for i in range(model.config.num_hidden_layers)\n","        ]\n","        + [\n","            \"bert.encoder.layer.\" + str(i) + \".attention.self.key\"\n","            for i in range(model.config.num_hidden_layers)\n","        ]\n","        + [\n","            \"bert.encoder.layer.\" + str(i) + \".attention.self.value\"\n","            for i in range(model.config.num_hidden_layers)\n","        ]\n","        + [\n","            \"bert.encoder.layer.\" + str(i) + \".attention.output.dense\"\n","            for i in range(model.config.num_hidden_layers)\n","        ]\n","    )\n","    \n","    peft_config = LoraConfig(\n","        task_type=TaskType.TOKEN_CLS, inference_mode=False, r=16, lora_alpha=16, lora_dropout=0.1, bias=\"all\"\n","    )\n","\n","    lora_config = LoraConfig(\n","        r=16,\n","        lora_alpha=32,\n","        lora_dropout=0.05,\n","        task_type=task_type,\n","        bias=\"none\",\n","        target_modules=target_modules,\n","    )\n","\n","    model = get_peft_model(model, lora_config)\n","\n","    _, metrics, elapsed_training = fine_tune_model(\n","        model_name, dataset, model, training_args\n","    )\n","    print(f\"Training time: {elapsed_training}\")\n","    current_time = time.strftime(\"%Y-%m-%d-%H-%M-%S\")\n","    with open(\"results.csv\", \"a\") as f:\n","        f.write(\n","            f\"{current_time},{model_name},SSJ500-NER, {metrics},{elapsed_training}\\n\"\n","        )\n","    print(f\"Training time: {elapsed_training}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-03T00:13:48.264369Z","iopub.status.busy":"2024-05-03T00:13:48.263948Z","iopub.status.idle":"2024-05-03T00:17:04.343895Z","shell.execute_reply":"2024-05-03T00:17:04.342959Z","shell.execute_reply.started":"2024-05-03T00:13:48.264340Z"},"trusted":true},"outputs":[],"source":["run_lora_bert(dataset = {\"train\": train_dataset, \"val\": val_dataset, \"test\": test_dataset})"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-02T22:23:25.817289Z","iopub.status.busy":"2024-05-02T22:23:25.816578Z","iopub.status.idle":"2024-05-02T22:23:25.829406Z","shell.execute_reply":"2024-05-02T22:23:25.828227Z","shell.execute_reply.started":"2024-05-02T22:23:25.817255Z"},"trusted":true},"outputs":[],"source":["def run_prefix_tune_sloberta(dataset):\n","    model_name = \"EMBEDDIA/sloberta\"\n","\n","    task_type = TaskType.TOKEN_CLS\n","    training_args = TrainingArguments(\n","        output_dir=f\"ner_prefix_tunning_finetuned_{model_name}\",\n","        learning_rate=1e-2,\n","        per_device_train_batch_size=32,\n","        per_device_eval_batch_size=32,\n","        num_train_epochs=3,\n","        weight_decay=0.01,\n","        evaluation_strategy=\"epoch\",\n","        save_strategy=\"epoch\",\n","        load_best_model_at_end=True,\n","    )\n","    model = AutoModelForTokenClassification.from_pretrained(\n","        model_name, num_labels=num_labels, id2label=id2label, label2id=label2id\n","    )\n","\n","    target_modules = (\n","        [\n","            \"roberta.encoder.layer.\" + str(i) + \".attention.self.query\"\n","            for i in range(model.config.num_hidden_layers)\n","        ]\n","        + [\n","            \"roberta.encoder.layer.\" + str(i) + \".attention.self.key\"\n","            for i in range(model.config.num_hidden_layers)\n","        ]\n","        + [\n","            \"roberta.encoder.layer.\" + str(i) + \".attention.self.value\"\n","            for i in range(model.config.num_hidden_layers)\n","        ]\n","        + [\n","            \"roberta.encoder.layer.\" + str(i) + \".attention.output.dense\"\n","            for i in range(model.config.num_hidden_layers)\n","        ]\n","    )\n","\n","    prefix_config = PrefixTuningConfig(task_type=\"TOKEN_CLS\", num_virtual_tokens=20)\n","\n","    model = get_peft_model(model, prefix_config)\n","\n","    _, metrics, elapsed_training = fine_tune_model(\n","        model_name, dataset, model, training_args\n","    )\n","    print(f\"Training time: {elapsed_training}\")\n","    current_time = time.strftime(\"%Y-%m-%d-%H-%M-%S\")\n","    with open(\"results.csv\", \"a\") as f:\n","        f.write(\n","            f\"{current_time},{model_name},SSJ500-NER, {metrics},{elapsed_training}\\n\"\n","        )"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-02T22:23:28.986322Z","iopub.status.busy":"2024-05-02T22:23:28.985436Z","iopub.status.idle":"2024-05-02T22:25:35.259769Z","shell.execute_reply":"2024-05-02T22:25:35.258668Z","shell.execute_reply.started":"2024-05-02T22:23:28.986288Z"},"trusted":true},"outputs":[],"source":["run_prefix_tune_sloberta(dataset = {\"train\": train_dataset, \"val\": val_dataset, \"test\": test_dataset})"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-03T00:19:05.522067Z","iopub.status.busy":"2024-05-03T00:19:05.521620Z","iopub.status.idle":"2024-05-03T00:19:05.534420Z","shell.execute_reply":"2024-05-03T00:19:05.533009Z","shell.execute_reply.started":"2024-05-03T00:19:05.522036Z"},"trusted":true},"outputs":[],"source":["def run_prefix_tune_bert(dataset):\n","    model_name = \"bert-base-multilingual-cased\"\n","\n","    task_type = TaskType.TOKEN_CLS\n","    training_args = TrainingArguments(\n","        output_dir=f\"ner_prefix_tunning_finetuned_{model_name}\",\n","        learning_rate=1e-2,\n","        per_device_train_batch_size=32,\n","        per_device_eval_batch_size=32,\n","        num_train_epochs=3,\n","        weight_decay=0.01,\n","        evaluation_strategy=\"epoch\",\n","        save_strategy=\"epoch\",\n","        load_best_model_at_end=True,\n","    )\n","    model = AutoModelForTokenClassification.from_pretrained(\n","        model_name, num_labels=num_labels, id2label=id2label, label2id=label2id\n","    )\n","\n","    target_modules = (\n","        [\n","            \"bert.encoder.layer.\" + str(i) + \".attention.self.query\"\n","            for i in range(model.config.num_hidden_layers)\n","        ]\n","        + [\n","            \"bert.encoder.layer.\" + str(i) + \".attention.self.key\"\n","            for i in range(model.config.num_hidden_layers)\n","        ]\n","        + [\n","            \"bert.encoder.layer.\" + str(i) + \".attention.self.value\"\n","            for i in range(model.config.num_hidden_layers)\n","        ]\n","        + [\n","            \"bert.encoder.layer.\" + str(i) + \".attention.output.dense\"\n","            for i in range(model.config.num_hidden_layers)\n","        ]\n","    )\n","\n","    prefix_config = PrefixTuningConfig(task_type=\"TOKEN_CLS\", num_virtual_tokens=20)\n","\n","    model = get_peft_model(model, prefix_config)\n","\n","    _, metrics, elapsed_training = fine_tune_model(\n","        model_name, dataset, model, training_args\n","    )\n","    print(f\"Training time: {elapsed_training}\")\n","    current_time = time.strftime(\"%Y-%m-%d-%H-%M-%S\")\n","    with open(\"results.csv\", \"a\") as f:\n","        f.write(\n","            f\"{current_time},{model_name},SSJ500-NER, {metrics},{elapsed_training}\\n\"\n","        )"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-03T00:19:10.913557Z","iopub.status.busy":"2024-05-03T00:19:10.912690Z","iopub.status.idle":"2024-05-03T00:21:55.720373Z","shell.execute_reply":"2024-05-03T00:21:55.718994Z","shell.execute_reply.started":"2024-05-03T00:19:10.913525Z"},"trusted":true},"outputs":[],"source":["run_prefix_tune_bert(dataset = {\"train\": train_dataset, \"val\": val_dataset, \"test\": test_dataset})"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-03T00:23:34.316203Z","iopub.status.busy":"2024-05-03T00:23:34.315305Z","iopub.status.idle":"2024-05-03T00:23:34.334468Z","shell.execute_reply":"2024-05-03T00:23:34.333395Z","shell.execute_reply.started":"2024-05-03T00:23:34.316166Z"},"trusted":true},"outputs":[],"source":["def run_ia3_sloberta(dataset):\n","    model_name = \"EMBEDDIA/sloberta\"\n","    task_type = TaskType.TOKEN_CLS\n","    training_args = TrainingArguments(\n","        output_dir=f\"ner-ia3-{model_name}\",\n","        learning_rate=1e-2,\n","        per_device_train_batch_size=32,\n","        per_device_eval_batch_size=32,\n","        num_train_epochs=3,\n","        weight_decay=0.01,\n","        evaluation_strategy=\"epoch\",\n","        save_strategy=\"epoch\",\n","        load_best_model_at_end=True,\n","    )\n","    model = AutoModelForTokenClassification.from_pretrained(\n","        model_name, num_labels=num_labels, id2label=id2label, label2id=label2id\n","    )\n","    model = prepare_model_for_kbit_training(model, task_type)\n","\n","    feed_forward_modules = [\n","        \"roberta.encoder.layer.\" + str(i) + \".intermediate.dense\"\n","        for i in range(model.config.num_hidden_layers)\n","    ]\n","\n","    target_modules = (\n","        [\n","            \"roberta.encoder.layer.\" + str(i) + \".attention.self.query\"\n","            for i in range(model.config.num_hidden_layers)\n","        ]\n","        + [\n","            \"roberta.encoder.layer.\" + str(i) + \".attention.self.key\"\n","            for i in range(model.config.num_hidden_layers)\n","        ]\n","        + [\n","            \"roberta.encoder.layer.\" + str(i) + \".attention.self.value\"\n","            for i in range(model.config.num_hidden_layers)\n","        ]\n","        + [\n","            \"roberta.encoder.layer.\" + str(i) + \".attention.output.dense\"\n","            for i in range(model.config.num_hidden_layers)\n","        ]\n","        + feed_forward_modules\n","    )\n","\n","    ia3_config = IA3Config(task_type=task_type, feedforward_modules = feed_forward_modules, target_modules=target_modules)\n","\n","    model = get_peft_model(model, ia3_config)\n","\n","    _, metrics, elapsed_training = fine_tune_model(\n","        model_name, dataset, model, training_args\n","    )\n","    print(f\"Training time: {elapsed_training}\")\n","    current_time = time.strftime(\"%Y-%m-%d-%H-%M-%S\")\n","    with open(\"results.csv\", \"a\") as f:\n","        f.write(\n","            f\"{current_time},{model_name},Sentinews, {metrics},{elapsed_training}\\n\"\n","        )"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-03T00:23:37.179203Z","iopub.status.busy":"2024-05-03T00:23:37.178845Z","iopub.status.idle":"2024-05-03T00:23:44.831721Z","shell.execute_reply":"2024-05-03T00:23:44.830045Z","shell.execute_reply.started":"2024-05-03T00:23:37.179176Z"},"trusted":true},"outputs":[],"source":["run_ia3_sloberta(dataset = {\"train\": train_dataset, \"val\": val_dataset, \"test\": test_dataset})"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-03T00:43:17.364862Z","iopub.status.busy":"2024-05-03T00:43:17.364141Z","iopub.status.idle":"2024-05-03T00:43:17.376705Z","shell.execute_reply":"2024-05-03T00:43:17.375657Z","shell.execute_reply.started":"2024-05-03T00:43:17.364832Z"},"trusted":true},"outputs":[],"source":["def run_ia3_bert(dataset):\n","    model_name = \"bert-base-multilingual-cased\"\n","    task_type = TaskType.TOKEN_CLS\n","    training_args = TrainingArguments(\n","        output_dir=f\"ner-ia3-{model_name}\",\n","        learning_rate=1e-1,\n","        per_device_train_batch_size=32,\n","        per_device_eval_batch_size=32,\n","        num_train_epochs=3,\n","        weight_decay=0.01,\n","        evaluation_strategy=\"epoch\",\n","        save_strategy=\"epoch\",\n","        load_best_model_at_end=True,\n","    )\n","    model = AutoModelForTokenClassification.from_pretrained(\n","        model_name, num_labels=9, id2label=id2label, label2id=label2id\n","    )\n","    model = prepare_model_for_kbit_training(model, task_type)\n","\n","    feed_forward_modules = [\n","        \"roberta.encoder.layer.\" + str(i) + \".intermediate.dense\"\n","        for i in range(model.config.num_hidden_layers)\n","    ]\n","\n","    target_modules = (\n","        [\n","            \"bert.encoder.layer.\" + str(i) + \".attention.self.query\"\n","            for i in range(model.config.num_hidden_layers)\n","        ]\n","        + [\n","            \"bert.encoder.layer.\" + str(i) + \".attention.self.key\"\n","            for i in range(model.config.num_hidden_layers)\n","        ]\n","        + [\n","            \"bert.encoder.layer.\" + str(i) + \".attention.self.value\"\n","            for i in range(model.config.num_hidden_layers)\n","        ]\n","        + [\n","            \"bert.encoder.layer.\" + str(i) + \".attention.output.dense\"\n","            for i in range(model.config.num_hidden_layers)\n","        ]\n","        + feed_forward_modules\n","    )\n","\n","    ia3_config = IA3Config(task_type=task_type, feedforward_modules = feed_forward_modules, target_modules=target_modules)\n","\n","    model = get_peft_model(model, ia3_config)\n","\n","    _, metrics, elapsed_training = fine_tune_model(\n","        model_name, dataset, model, training_args\n","    )\n","    print(f\"Training time: {elapsed_training}\")\n","    current_time = time.strftime(\"%Y-%m-%d-%H-%M-%S\")\n","    with open(\"results.csv\", \"a\") as f:\n","        f.write(\n","            f\"{current_time},{model_name},Sentinews, {metrics},{elapsed_training}\\n\"\n","        )"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-03T00:43:20.407551Z","iopub.status.busy":"2024-05-03T00:43:20.406774Z","iopub.status.idle":"2024-05-03T00:46:11.916974Z","shell.execute_reply":"2024-05-03T00:46:11.915874Z","shell.execute_reply.started":"2024-05-03T00:43:20.407522Z"},"trusted":true},"outputs":[],"source":["run_ia3_bert(dataset = {\"train\": train_dataset, \"val\": val_dataset, \"test\": test_dataset})"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30699,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
