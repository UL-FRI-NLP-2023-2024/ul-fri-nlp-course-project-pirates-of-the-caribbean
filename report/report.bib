@article{Demsar2016BalancedMixture,
    title = {{A Balanced Mixture of Antagonistic Pressures Promotes the Evolution of Parallel Movement}},
    year = {2016},
    journal = {Scientific Reports},
    author = {Dem{\v{s}}ar, Jure and {\v{S}}trumbelj, Erik and Lebar Bajec, Iztok},
    volume = {6},
    doi = {10.1038/srep39428}
}

@article{Demsar2017LinguisticEvolution,
    title = {{Evolution of Collective Behaviour in an Artificial World Using Linguistic Fuzzy Rule-Based Systems}},
    year = {2017},
    journal = {PLoS ONE},
    author = {Dem{\v{s}}ar, Jure and Lebar Bajec, Iztok},
    number = {1},
    pages = {1--20},
    volume = {12},
    doi = {10.1371/journal.pone.0168876}
}
@inproceedings{
hu2022lora,
title={Lo{RA}: Low-Rank Adaptation of Large Language Models},
author={Edward J Hu and yelong shen and Phillip Wallis and Zeyuan Allen-Zhu and Yuanzhi Li and Shean Wang and Lu Wang and Weizhu Chen},
booktitle={International Conference on Learning Representations},
year={2022},
url={https://openreview.net/forum?id=nZeVKeeFYf9}
}
@misc{zhang2023increlora,
      title={IncreLoRA: Incremental Parameter Allocation Method for Parameter-Efficient Fine-tuning}, 
      author={Feiyu Zhang and Liangzhi Li and Junhao Chen and Zhouqiang Jiang and Bowen Wang and Yiming Qian},
      year={2023},
      eprint={2308.12043},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
 @misc{11356/1397,
 title = {Slovenian {RoBERTa} contextual embeddings model: {SloBERTa} 2.0},
 author = {Ul{\v c}ar, Matej and Robnik-{\v S}ikonja, Marko},
 url = {http://hdl.handle.net/11356/1397},
 note = {Slovenian language resource repository {CLARIN}.{SI}},
 copyright = {Creative Commons - Attribution-{ShareAlike} 4.0 International ({CC} {BY}-{SA} 4.0)},
 issn = {2820-4042},
 year = {2021} }
@misc{xu2023qalora,
      title={QA-LoRA: Quantization-Aware Low-Rank Adaptation of Large Language Models}, 
      author={Yuhui Xu and Lingxi Xie and Xiaotao Gu and Xin Chen and Heng Chang and Hengheng Zhang and Zhengsu Chen and Xiaopeng Zhang and Qi Tian},
      year={2023},
      eprint={2309.14717},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@misc{sheng2023slora,
      title={S-LoRA: Serving Thousands of Concurrent LoRA Adapters}, 
      author={Ying Sheng and Shiyi Cao and Dacheng Li and Coleman Hooper and Nicholas Lee and Shuo Yang and Christopher Chou and Banghua Zhu and Lianmin Zheng and Kurt Keutzer and Joseph E. Gonzalez and Ion Stoica},
      year={2023},
      eprint={2311.03285},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@misc{gao2024higher,
      title={Higher Layers Need More LoRA Experts}, 
      author={Chongyang Gao and Kezhen Chen and Jinmeng Rao and Baochen Sun and Ruibo Liu and Daiyi Peng and Yawen Zhang and Xiaoyuan Guo and Jie Yang and VS Subrahmanian},
      year={2024},
      eprint={2402.08562},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@misc{bitfit,
  doi = {10.48550/ARXIV.2106.10199},
  url = {https://arxiv.org/abs/2106.10199},
  author = {Zaken,  Elad Ben and Ravfogel,  Shauli and Goldberg,  Yoav},
  keywords = {Machine Learning (cs.LG),  Computation and Language (cs.CL),  FOS: Computer and information sciences,  FOS: Computer and information sciences},
  title = {BitFit: Simple Parameter-efficient Fine-tuning for Transformer-based Masked Language-models},
  publisher = {arXiv},
  archivePrefix={arXiv},
  year = {2021},
  copyright = {Creative Commons Attribution 4.0 International}
}

@misc{survey,
  doi = {10.48550/ARXIV.2312.12148},
  url = {https://arxiv.org/abs/2312.12148},
  author = {Xu,  Lingling and Xie,  Haoran and Qin,  Si-Zhao Joe and Tao,  Xiaohui and Wang,  Fu Lee},
  keywords = {Computation and Language (cs.CL),  FOS: Computer and information sciences,  FOS: Computer and information sciences},
  title = {Parameter-Efficient Fine-Tuning Methods for Pretrained Language Models: A Critical Review and Assessment},
  publisher = {arXiv},
  year = {2023},
  copyright = {Creative Commons Attribution Non Commercial Share Alike 4.0 International}
}

@misc{us-bitfit,
  doi = {10.48550/ARXIV.2305.16597},
  url = {https://arxiv.org/abs/2305.16597},
  author = {Lawton,  Neal and Kumar,  Anoop and Thattai,  Govind and Galstyan,  Aram and Steeg,  Greg Ver},
  keywords = {Computation and Language (cs.CL),  Artificial Intelligence (cs.AI),  Machine Learning (cs.LG),  FOS: Computer and information sciences,  FOS: Computer and information sciences,  I.2.7},
  title = {Neural Architecture Search for Parameter-Efficient Fine-tuning of Large Pre-trained Language Models},
  publisher = {arXiv},
  year = {2023},
  copyright = {Creative Commons Attribution Non Commercial Share Alike 4.0 International}
}

@misc{prompt_tuning,
  doi = {10.48550/ARXIV.2104.08691},
  url = {https://arxiv.org/abs/2104.08691},
  author = {Lester,  Brian and Al-Rfou,  Rami and Constant,  Noah},
  keywords = {Computation and Language (cs.CL),  FOS: Computer and information sciences,  FOS: Computer and information sciences},
  title = {The Power of Scale for Parameter-Efficient Prompt Tuning},
  publisher = {arXiv},
  year = {2021},
  copyright = {arXiv.org perpetual,  non-exclusive license}
}

@misc{prefix_tuning,
  doi = {10.48550/ARXIV.2101.00190},
  url = {https://arxiv.org/abs/2101.00190},
  author = {Li,  Xiang Lisa and Liang,  Percy},
  keywords = {Computation and Language (cs.CL),  FOS: Computer and information sciences,  FOS: Computer and information sciences},
  title = {Prefix-Tuning: Optimizing Continuous Prompts for Generation},
  publisher = {arXiv},
  year = {2021},
  copyright = {arXiv.org perpetual,  non-exclusive license}
}

@article{carbon_footprint,
author = {Jiang, Peng and Sonne, Christian and Li, Wangliang and You, Fengqi and You, Siming},
year = {2024},
month = {04},
pages = {},
title = {Preventing the Immense Increase in the Life-Cycle Energy and Carbon Footprints of LLM-Powered Intelligent Chatbots},
journal = {Engineering},
doi = {10.1016/j.eng.2024.04.002}
}

@misc{ia3,
  doi = {10.48550/ARXIV.2205.05638},
  url = {https://arxiv.org/abs/2205.05638},
  author = {Liu,  Haokun and Tam,  Derek and Muqeeth,  Mohammed and Mohta,  Jay and Huang,  Tenghao and Bansal,  Mohit and Raffel,  Colin},
  keywords = {Machine Learning (cs.LG),  Artificial Intelligence (cs.AI),  Computation and Language (cs.CL),  FOS: Computer and information sciences,  FOS: Computer and information sciences},
  title = {Few-Shot Parameter-Efficient Fine-Tuning is Better and Cheaper than In-Context Learning},
  publisher = {arXiv},
  year = {2022},
  copyright = {Creative Commons Attribution 4.0 International}
}